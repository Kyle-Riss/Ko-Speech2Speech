# EchoStream êµ¬í˜„ ë¡œë“œë§µ

**ëª©í‘œ**: StreamSpeech ë¶„ì„ ê¸°ë°˜ EchoStream ì™„ì„± êµ¬í˜„

**ê¸°ê°„**: 6 Phase (ì²´ê³„ì  ë‹¨ê³„ë³„ êµ¬í˜„)

**í•µì‹¬ ê°œì„ ì‚¬í•­**:
1. Conformer â†’ Emformer (O(TÂ²) â†’ O(1))
2. Wait-k â†’ Word-Level Streaming (1200ms â†’ 100ms)
3. ë‹¨ì¼ ì²­í¬ â†’ Multi-chunk Training (ìœ ì—°í•œ ë ˆì´í„´ì‹œ)
4. ë‹¨ìˆœ í•™ìŠµ â†’ Multi-task Learning (í’ˆì§ˆ í–¥ìƒ)

---

## ğŸ“ StreamSpeech êµ¬ì¡° ë¶„ì„

### í•µì‹¬ ë””ë ‰í† ë¦¬

```
StreamSpeech_analysis/
â”œâ”€â”€ researches/ctc_unity/          # StreamSpeech ë©”ì¸ êµ¬í˜„
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ streamspeech_model.py  # ë©”ì¸ ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ s2s_conformer.py       # Conformer ê¸°ë°˜
â”‚   â”‚   â””â”€â”€ s2t_conformer.py       # Encoder
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ conformer_layer.py     # Conformer ë ˆì´ì–´
â”‚   â”‚   â”œâ”€â”€ ctc_decoder_with_transformer_layer.py  # ST CTC
â”‚   â”‚   â”œâ”€â”€ ctc_transformer_unit_decoder.py        # Unit Decoder
â”‚   â”‚   â”œâ”€â”€ transformer_decoder.py                 # MT Decoder
â”‚   â”‚   â””â”€â”€ transformer_encoder.py                 # T2U Encoder
â”‚   â”œâ”€â”€ criterions/
â”‚   â”‚   â””â”€â”€ speech_to_speech_ctc_asr_st_criterion.py  # Multi-task Loss
â”‚   â””â”€â”€ tasks/
â”‚       â””â”€â”€ speech_to_speech_ctc.py                # Task ì •ì˜
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ speech_to_speech.streamspeech.agent.py    # S2ST Agent
â”‚   â”œâ”€â”€ ctc_decoder.py                             # CTC ë””ì½”ë”
â”‚   â””â”€â”€ tts/codehifigan.py                        # Vocoder
â””â”€â”€ fairseq/                                       # ê¸°ë°˜ í”„ë ˆì„ì›Œí¬
```

---

## ğŸ¯ Phase 1: StreamSpeech í•µì‹¬ êµ¬ì¡° ë¶„ì„ [IN PROGRESS]

### ëª©í‘œ
StreamSpeechì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´í•´ ë° ë§¤í•‘

### ì‘ì—…

#### 1.1 ëª¨ë¸ êµ¬ì¡° ë¶„ì„ âœ…

**íŒŒì¼**: `researches/ctc_unity/models/streamspeech_model.py`

**í•µì‹¬ ì»´í¬ë„ŒíŠ¸**:
```python
class StreamSpeechModel(ChunkS2UTConformerModel):
    def __init__(self, encoder, multitask_decoders, args):
        # 1. Speech Encoder (Chunk-based Conformer)
        self.encoder
        
        # 2. Multi-task Decoders
        self.multitask_decoders = {
            'source_unigram': ASR CTC Decoder,
            'ctc_target_unigram': ST CTC Decoder,
            'target_unigram': MT Decoder,
        }
        
        # 3. T2U Encoder (Optional)
        self.synthesizer_encoder
        
        # 4. Unit Decoder
        self.decoder  # CTCTransformerUnitDecoder
```

**EchoStream ë§¤í•‘**:
```python
class EchoStreamModel(nn.Module):
    def __init__(self, ...):
        # 1. Emformer Encoder (ëŒ€ì²´!)
        self.encoder = EchoStreamSpeechEncoder(...)
        
        # 2. Multi-task Decoders (ë™ì¼)
        self.asr_ctc_decoder = CTCDecoder(...)
        self.st_ctc_decoder = CTCDecoderWithTransformerLayer(...)
        self.mt_decoder = TransformerMTDecoder(...)
        
        # 3. T2U Encoder (ë™ì¼, 0 layers)
        # ìƒëµ (ì§ì ‘ ì—°ê²°)
        
        # 4. Unit Decoder (ë™ì¼)
        self.unit_decoder = CTCTransformerUnitDecoder(...)
        
        # 5. Vocoder (ë™ì¼)
        self.vocoder = CodeHiFiGANVocoder(...)
```

---

#### 1.2 ë””ì½”ë” êµ¬ì¡° ë¶„ì„

**ASR CTC Decoder**:
```python
# modules/ctc_decoder.py (fairseq)
class CTCDecoder(nn.Module):
    def __init__(self, embed_dim, vocab_size):
        self.output_projection = nn.Linear(embed_dim, vocab_size)
    
    def forward(self, encoder_out):
        logits = self.output_projection(encoder_out)
        return logits
```

**ST CTC Decoder**:
```python
# modules/ctc_decoder_with_transformer_layer.py
class CTCDecoderWithTransformerLayer(nn.Module):
    def __init__(self, ..., num_layers=2, unidirectional=True):
        # Transformer layers (2L)
        self.layers = nn.ModuleList([
            TransformerEncoderLayer(...) for _ in range(num_layers)
        ])
        
        # CTC projection
        self.output_projection = nn.Linear(embed_dim, vocab_size)
    
    def forward(self, encoder_out, incremental_state=None):
        x = encoder_out
        
        # Transformer layers with causal mask
        for layer in self.layers:
            x = layer(x, self_attn_mask=future_mask if unidirectional else None)
        
        # CTC projection
        logits = self.output_projection(x)
        return logits
```

**MT Decoder**:
```python
# modules/transformer_decoder.py
class TransformerDecoder(TransformerDecoderBase):
    def __init__(self, ..., num_layers=4):
        # Standard Transformer Decoder
        # - Self-attention (causal)
        # - Cross-attention (encoder_out)
        # - Feed-forward
        pass
```

**Unit Decoder**:
```python
# modules/ctc_transformer_unit_decoder.py
class CTCTransformerUnitDecoder(TransformerUnitDecoder):
    def __init__(self, ..., num_layers=6, ctc_upsample_ratio=5):
        # CTC upsampling
        self.ctc_upsample_ratio = 5
        
        # Transformer layers (6L)
        self.layers = ...
        
        # Multi-frame prediction
        self.output_projection = nn.Linear(embed_dim, num_units * n_frames_per_step)
```

---

#### 1.3 ì •ì±… ë¶„ì„

**Agent íŒŒì¼**: `agent/speech_to_speech.streamspeech.agent.py`

**í•µì‹¬ ì •ì±…**:
```python
def policy(self):
    # 1. Feature extraction
    feature = self.feature_extractor(self.states.source)
    
    # 2. Encoder forward
    encoder_outs = self.model.forward_encoder(...)
    
    # 3. ASR CTC
    finalized_asr = self.asr_ctc_generator.generate(encoder_outs)
    src_ctc_length = asr_tokens.size(-1)
    
    # 4. ST CTC
    finalized_st = self.st_ctc_generator.generate(encoder_outs)
    tgt_ctc_length = st_tokens.size(-1)
    
    # 5. READ/WRITE ì •ì±…
    if (
        src_ctc_length < self.src_ctc_prefix_length + self.stride_n
        or tgt_ctc_length < self.tgt_ctc_prefix_length + self.stride_n
    ):
        return ReadAction()  # READ
    
    # 6. MT Decoder
    new_subword_tokens = (
        (tgt_ctc_length - self.lagging_k1) // self.stride_n
    ) * self.stride_n
    
    finalized_mt = self.generator_mt.generate_decoder(
        ...,
        max_new_tokens=new_subword_tokens  # Alignment-guided!
    )
    
    # 7. Unit Decoder
    finalized = self.ctc_generator.generate(mt_output)
    
    # 8. Vocoder
    wav = self.vocoder(units)
    
    # 9. WRITE
    return WriteAction(SpeechSegment(content=wav))
```

---

## ğŸš€ Phase 2: Word-Level Streaming ëª¨ë“ˆ êµ¬í˜„ [PENDING]

### ëª©í‘œ
StreamSpeech ì •ì±…ì„ Word-Levelë¡œ ê°œì„ 

### ì‘ì—…

#### 2.1 WordBoundaryDetector êµ¬í˜„

**íŒŒì¼**: `models/word_boundary_detector.py`

```python
class WordBoundaryDetector:
    """
    ASR CTC + SentencePieceë¥¼ ì‚¬ìš©í•œ ë‹¨ì–´ ê²½ê³„ íƒì§€.
    
    StreamSpeech ê°œì„ :
    - StreamSpeech: stride_n ê¸°ë°˜ (ê³ ì • í† í° ìˆ˜)
    - EchoStream: ë‹¨ì–´ ê²½ê³„ ê¸°ë°˜ (ë™ì )
    """
    
    def __init__(self, emformer_encoder, asr_ctc_decoder, tokenizer):
        self.encoder = emformer_encoder
        self.asr_ctc = asr_ctc_decoder
        self.tokenizer = tokenizer
        
        # Cache (Emformer)
        self.encoder_cache = {}
        self.partial_word = ""
        
    def process_segment(self, audio_segment):
        """
        ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ë° ë‹¨ì–´ ê²½ê³„ íƒì§€.
        
        Returns:
            None: ë‹¨ì–´ ë¯¸ì™„ì„±
            Dict: ì™„ì„±ëœ ë‹¨ì–´
        """
        # 1. Emformer encoding (with cache)
        encoder_out, self.encoder_cache = self.encoder(
            audio_segment,
            cache=self.encoder_cache
        )
        
        # 2. ASR CTC decoding
        asr_logits = self.asr_ctc(encoder_out)
        asr_tokens = asr_logits.argmax(dim=-1)
        
        # 3. CTC collapse
        collapsed_tokens = self._ctc_collapse(asr_tokens)
        
        # 4. Decode
        new_text = self.tokenizer.decode(collapsed_tokens)
        
        # 5. Word boundary check
        if self._is_word_boundary(new_text):
            word = self.partial_word + new_text.rstrip("â– ")
            self.partial_word = ""
            
            return {
                'word': word,
                'encoder_out': encoder_out,
                'is_complete': True
            }
        else:
            self.partial_word += new_text
            return None
    
    def _is_word_boundary(self, text):
        """SentencePiece â– í† í° ì²´í¬"""
        return (
            text.endswith("â–") or
            text.endswith(" ") or
            text.endswith((".", ",", "!", "?"))
        )
```

---

#### 2.2 WordLevelTranslator êµ¬í˜„

**íŒŒì¼**: `models/word_level_translator.py`

```python
class WordLevelTranslator:
    """
    ë‹¨ì–´ ë‹¨ìœ„ ë²ˆì—­.
    
    StreamSpeechì™€ ì°¨ì´:
    - StreamSpeech: ì²­í¬ ë‹¨ìœ„ batch ìƒì„±
    - EchoStream: ë‹¨ì–´ ë‹¨ìœ„ incremental ìƒì„±
    """
    
    def __init__(self, st_ctc, mt_decoder, unit_decoder, vocoder):
        self.st_ctc = st_ctc
        self.mt_decoder = mt_decoder
        self.unit_decoder = unit_decoder
        self.vocoder = vocoder
        
        # Incremental state (StreamSpeechì™€ ë™ì¼)
        self.mt_incremental_state = {}
        self.prev_mt_tokens = None
    
    def translate_word(self, encoder_out, source_word):
        """
        ë‹¨ì–´ ë²ˆì—­ (StreamSpeech ì •ì±… í™œìš©).
        
        StreamSpeechì˜ MT Decoder ë¡œì§ ì°¨ìš©:
        - max_new_tokens ê³„ì‚°
        - Incremental state ê´€ë¦¬
        """
        # 1. ST CTC
        st_logits = self.st_ctc(encoder_out)
        st_tokens = st_logits.argmax(dim=-1)
        st_tokens = self._ctc_collapse(st_tokens)
        
        # 2. MT Decoder (incremental)
        # StreamSpeechì˜ alignment-guided token calculation ì°¨ìš©
        max_new_tokens = len(st_tokens) + 2
        
        mt_output = self.mt_decoder(
            prev_output_tokens=self.prev_mt_tokens,
            encoder_out=encoder_out,
            incremental_state=self.mt_incremental_state,
            max_new_tokens=max_new_tokens
        )
        
        # 3. Extract new tokens
        if self.prev_mt_tokens is not None:
            new_mt_tokens = mt_output['tokens'][len(self.prev_mt_tokens):]
        else:
            new_mt_tokens = mt_output['tokens']
        
        self.prev_mt_tokens = mt_output['tokens']
        
        # 4. Unit Decoder
        unit_output = self.unit_decoder(mt_output['decoder_out'])
        units = unit_output['units']
        
        # 5. Vocoder
        waveform = self.vocoder(units.unsqueeze(0))
        
        return {
            'translation': self.tokenizer.decode(new_mt_tokens),
            'units': units,
            'waveform': waveform
        }
```

---

## ğŸ“š Phase 3: Multi-task í•™ìŠµ êµ¬í˜„ [PENDING]

### ëª©í‘œ
StreamSpeechì˜ Multi-task Learning ì°¨ìš©

### ì‘ì—…

#### 3.1 Multi-task Criterion êµ¬í˜„

**ì°¸ê³ **: `criterions/speech_to_speech_ctc_asr_st_criterion.py`

```python
class EchoStreamMultiTaskCriterion(nn.Module):
    """
    StreamSpeechì˜ Multi-task Learning ì°¨ìš©.
    
    L = L_asr + L_st + L_mt + L_unit
    """
    
    def forward(self, model, sample):
        # 1. Forward pass
        output = model(
            src_tokens=sample['net_input']['src_tokens'],
            src_lengths=sample['net_input']['src_lengths'],
            prev_output_tokens=sample['prev_output_tokens']
        )
        
        # 2. ASR Loss (CTC)
        L_asr = self.compute_ctc_loss(
            output['asr_logits'],
            sample['source_text']
        )
        
        # 3. ST Loss (CTC)
        L_st = self.compute_ctc_loss(
            output['st_logits'],
            sample['target_text']
        )
        
        # 4. MT Loss (Cross-Entropy)
        L_mt = self.compute_ce_loss(
            output['mt_logits'],
            sample['target_text']
        )
        
        # 5. Unit Loss (CTC)
        L_unit = self.compute_ctc_loss(
            output['unit_logits'],
            sample['target_units']
        )
        
        # 6. Total Loss
        L_total = L_asr + L_st + L_mt + L_unit
        
        return L_total, {
            'loss': L_total,
            'L_asr': L_asr,
            'L_st': L_st,
            'L_mt': L_mt,
            'L_unit': L_unit
        }
```

---

## ğŸ² Phase 4: Alignment-based ì •ì±… êµ¬í˜„ [PENDING]

### ëª©í‘œ
StreamSpeechì˜ ì •ë ¬ ê¸°ë°˜ READ/WRITE ì •ì±… í†µí•©

### ì‘ì—…

#### 4.1 Policy Module êµ¬í˜„

**ì°¸ê³ **: `agent/speech_to_speech.streamspeech.agent.py:480-509`

```python
class AlignmentBasedPolicy:
    """
    StreamSpeechì˜ alignment-based policy ì°¨ìš©.
    
    ì¡°ê±´:
    - |Ã‚| > |A|: ìƒˆ ASR í† í° ì¸ì‹
    - |Å¶| > |Y|: ìƒˆ ST í† í° ì˜ˆì¸¡
    """
    
    def __init__(self, stride_n=1):
        self.stride_n = stride_n
        
        # Previous lengths
        self.src_ctc_prefix_length = 0
        self.tgt_ctc_prefix_length = 0
    
    def should_write(self, asr_tokens, st_tokens):
        """
        WRITE ì—¬ë¶€ ê²°ì • (StreamSpeech ë¡œì§).
        
        Returns:
            (should_write, max_new_tokens)
        """
        src_ctc_length = asr_tokens.size(-1)
        tgt_ctc_length = st_tokens.size(-1)
        
        # StreamSpeech ì •ì±… (Line 485-489)
        if (
            src_ctc_length < self.src_ctc_prefix_length + self.stride_n
            or tgt_ctc_length < self.tgt_ctc_prefix_length + self.stride_n
        ):
            return False, 0  # READ
        
        # Update lengths
        self.src_ctc_prefix_length = max(src_ctc_length, self.src_ctc_prefix_length)
        self.tgt_ctc_prefix_length = max(tgt_ctc_length, self.tgt_ctc_prefix_length)
        
        # Calculate max_new_tokens (Line 496-498)
        max_new_tokens = (tgt_ctc_length // self.stride_n) * self.stride_n
        
        return True, max_new_tokens  # WRITE
```

---

## ğŸ”€ Phase 5: Multi-chunk í•™ìŠµ êµ¬í˜„ [PENDING]

### ëª©í‘œ
StreamSpeechì˜ Multi-chunk Training ì°¨ìš©

### ì‘ì—…

#### 5.1 Multi-chunk Sampler êµ¬í˜„

```python
class MultiChunkSampler:
    """
    StreamSpeechì˜ Multi-chunk Training.
    
    C ~ U(1, |X|)
    """
    
    def __init__(self, min_segment=1, max_segment=None):
        self.min_segment = min_segment
        self.max_segment = max_segment
    
    def sample_segment_length(self, audio_length):
        """
        ëœë¤ ì„¸ê·¸ë¨¼íŠ¸ í¬ê¸° ìƒ˜í”Œë§.
        
        Returns:
            segment_length: 1 ~ audio_length
        """
        max_len = self.max_segment or audio_length
        segment_length = random.randint(self.min_segment, max_len)
        
        return segment_length
```

#### 5.2 Training Loop ìˆ˜ì •

```python
class EchoStreamTrainer:
    def train_step(self, batch):
        # Multi-chunk: ëœë¤ ì„¸ê·¸ë¨¼íŠ¸ í¬ê¸°
        segment_length = self.sampler.sample_segment_length(
            batch['audio_length']
        )
        
        # Model forward with random segment length
        output = self.model(
            batch['audio'],
            segment_length=segment_length  # â† ë™ì !
        )
        
        # Multi-task loss
        loss, loss_dict = self.criterion(output, batch)
        
        return loss
```

---

## ğŸ­ Phase 6: í†µí•© Agent êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸ [PENDING]

### ëª©í‘œ
ëª¨ë“  ëª¨ë“ˆ í†µí•© ë° StreamSpeechì™€ ë¹„êµ

### ì‘ì—…

#### 6.1 EchoStream Word-Level Agent

```python
@entrypoint
class EchoStreamWordLevelAgent(SpeechToSpeechAgent):
    """
    StreamSpeech ì •ì±… + Emformer + Word-Level.
    
    ê°œì„ ì‚¬í•­:
    1. Conformer â†’ Emformer (O(1))
    2. Chunk 320ms â†’ Segment 40ms
    3. Wait-k â†’ Word boundary
    4. ì •ë ¬ ê¸°ë°˜ ì •ì±… ìœ ì§€
    """
    
    def __init__(self, args):
        super().__init__(args)
        
        # Components
        self.word_detector = WordBoundaryDetector(...)
        self.translator = WordLevelTranslator(...)
        self.recomposer = SentenceRecomposer(...)
        self.policy = AlignmentBasedPolicy(...)
    
    def policy(self):
        # 1. ì„¸ê·¸ë¨¼íŠ¸ ì½ê¸° (40ms)
        segment = self.states.source
        
        # 2. ë‹¨ì–´ ê²½ê³„ íƒì§€
        word_data = self.word_detector.process_segment(segment)
        
        if not word_data:
            return ReadAction()  # ë‹¨ì–´ ë¯¸ì™„ì„±
        
        # 3. StreamSpeech ì •ì±… ì²´í¬
        should_write, max_new_tokens = self.policy.should_write(
            asr_tokens=word_data['asr_tokens'],
            st_tokens=word_data['st_tokens']
        )
        
        if not should_write:
            return ReadAction()  # ì •ì±…: READ
        
        # 4. ë‹¨ì–´ ë²ˆì—­
        word_result = self.translator.translate_word(
            encoder_out=word_data['encoder_out'],
            source_word=word_data['word'],
            max_new_tokens=max_new_tokens  # StreamSpeech alignment!
        )
        
        # 5. ë¬¸ì¥ ì¬ì¡°í•© ì²´í¬
        sentence_result = self.recomposer.add_word(word_result)
        
        # 6. WRITE
        return WriteAction(
            SpeechSegment(
                content=sentence_result['waveform'],
                sample_rate=16000
            )
        )
```

---

## ğŸ“Š ìµœì¢… ë¹„êµ ëª©í‘œ

| ë©”íŠ¸ë¦­ | StreamSpeech | EchoStream (ëª©í‘œ) |
|--------|-------------|-------------------|
| Encoder | Conformer O(TÂ²) | Emformer O(1) |
| ì²« ì‘ë‹µ | 800ms | 100ms |
| ì •ì±… | Wait-k (ê³ ì •) | Word-Level (ë™ì ) |
| ë©”ëª¨ë¦¬ (60s) | 150MB | 6MB |
| RTF (60s) | 3.0 âŒ | 0.4 âœ… |
| í•™ìŠµ | Multi-task âœ… | Multi-task âœ… |
| ì²­í¬ í¬ê¸° | ê³ ì • 320ms | Multi-chunk âœ… |

---

## ğŸ“… ì¼ì •

- **Week 1**: Phase 1-2 (êµ¬ì¡° ë¶„ì„ + Word-Level)
- **Week 2**: Phase 3-4 (Multi-task + Policy)
- **Week 3**: Phase 5-6 (Multi-chunk + Integration)
- **Week 4**: Testing & Benchmarking

---

**í˜„ì¬ ì§„í–‰**: Phase 1 (êµ¬ì¡° ë¶„ì„ ì¤‘) âœ…

**ë‹¤ìŒ ë‹¨ê³„**: StreamSpeech ëª¨ë“ˆ ìƒì„¸ ë¶„ì„ ë° EchoStream ë§¤í•‘

