EchoStream - Real-time Speech-to-Speech Translation (Emformer + Units)

This repository contains a FastAPI-based real-time speech-to-speech translation system built around an Emformer encoder, multi-task decoders (ASR/ST/MT/Unit), and a CodeHiFiGAN-based vocoder. It supports low-latency streaming with MT incremental state, CTC gating, and duration-synchronized unit synthesis.

1) What you get

- Real-time server (FastAPI + WebSocket)
- Web client (server/static/index.html) and Python client (server/client_ws.py)
- Streaming-quality improvements: MT incremental state, ST-CTC gating, vocoder duration hop sync
- CPU-safe data loading path (librosa/soundfile fallback when torchaudio/torchcodec is unavailable)
- Mini training config with unit learning enabled (configs/echostream_config.mini.yaml)

2) Requirements

- OS: Linux/macOS (Apple Silicon works; CPU-only supported)
- Python: 3.10â€“3.12
- PortAudio/libsndfile/ffmpeg (for mic I/O; macOS: brew install portaudio libsndfile ffmpeg)
- Git

3) Clone

Shallow-clone only the working branch (recommended to avoid large history):

```bash
git clone --depth 1 --branch feature/streaming-mini-units https://github.com/Kyle-Riss/EchoStream.git
cd EchoStream
```

4) Python environment

```bash
python3 -m venv .venv
source .venv/bin/activate
python -m pip install --upgrade pip
pip install -r requirements.txt
```

If torchaudio is not usable (torchcodec missing), the project automatically falls back to soundfile/librosa in datasets/s2st_dataset.py.

5) Assets you must provide

We do not commit large assets. Prepare these on your machine (or host them as release artifacts and download via your preferred script).

- pretrain_models/
  - mHuBERT (e.g., pretrain_models/mHuBERT/mhubert_base_vp_en_es_fr_it3.pt)
  - CodeHiFiGAN vocoder (config.json + g_*.pt)
- data/
  - Manifests: train/dev/test tsv (e.g., data/train_sampled.units.tsv, data/dev_sampled.units.tsv, data/test_sampled.tsv)
  - Units: data/units/*.npy
  - gcmvn: data/gcmvn.npz
  - Dictionaries: data/src_unigram6000/spm_unigram_ko.txt, data/tgt_unigram6000/spm_unigram_en.txt

Place the assets according to the paths referenced in configs/echostream_config.yaml and configs/echostream_config.mini.yaml. If your paths differ, update the config files accordingly.

6) Configuration

- Inference config: configs/echostream_config.yaml
  - Points to global_cmvn, dictionaries, mHuBERT model, and (optional) vocoder paths
  - Sets streaming.ctc_threshold, and the server reads vocoder code_hop_size from the vocoder config.json
- Training (mini) config: configs/echostream_config.mini.yaml
  - Smaller Emformer/decoders
  - Emphasis on unit learning (multitask.unit_weight = 0.70)
  - data.units_root and data.load_tgt_units: true
  - streaming.ctc_threshold: 0.6
  - Uses train_sampled.units.tsv/dev_sampled.units.tsv/test_sampled.tsv by default

7) Quick health check

```bash
python -c "import torch, numpy; print('OK: torch', torch.__version__)"
```

8) Train (mini) - optional but recommended for quality

CPU-friendly run (DataLoader single worker to avoid shm issues):

```bash
source .venv/bin/activate
python scripts/train.py \
  --config configs/echostream_config.mini.yaml \
  --train-manifest /Users/you/EchoStream/data/train_sampled.units.tsv \
  --dev-manifest   /Users/you/EchoStream/data/dev_sampled.units.tsv \
  --save-dir checkpoints_mini \
  --num-workers 0
```

Notes:
- If you see torchcodec ImportError during data loading, the fallback in datasets/s2st_dataset.py uses librosa/soundfile automatically.
- If you still encounter shared memory errors on macOS, ensure num_workers=0 as above.

9) Start the server

```bash
source .venv/bin/activate
uvicorn server.fastapi_app:app --host 0.0.0.0 --port 8000
```

Endpoints:
- GET /health -> {"status":"ok"}
- GET /config -> current effective config
- Web UI -> http://127.0.0.1:8000/ (served from server/static/index.html)

10) Web UI (browser)

- Open http://127.0.0.1:8000/
- Click ë…¹ìŒ ì‹œì‘ to stream microphone audio
- The server returns synthesized translated audio (raw PCM), played in the page

11) Python WebSocket client (optional)

```bash
source .venv/bin/activate
python server/client_ws.py --host 127.0.0.1 --port 8000
```

12) Streaming-quality features

Already implemented:
- MT incremental state: decoder state persists across chunks
- CTC gating & whole-word policy: use ST-CTC confidence to gate updates
- Duration hop sync: server reads code_hop_size from vocoder config.json and extracts only the new audio segment based on durations

13) Unit learning rationale

For natural prosody and low-latency stability, the model predicts target unit sequences (mHuBERT K-means) in addition to text. This reduces pop/mix artifacts and improves chunk boundary stability. Use the *.units.tsv manifests to enable unit supervision.

14) Troubleshooting

- â€œTorchCodec is requiredâ€ during torchaudio.load:
  - Fixed by the built-in fallback: datasets/s2st_dataset.py tries torchaudio first, then soundfile/librosa.
- SHM/permissions on macOS:
  - Run training with --num-workers 0.
- Vocoder mismatch / pop-pop sounds:
  - Ensure vocoder config.json/g_*.pt match the unit settings (layer=11, km=1000) and that server uses duration prediction. Confirm code_hop_size alignment via the vocoder config.
- Unit loss stays 0:
  - Make sure you are using the *.units.tsv manifests and data.load_tgt_units: true.

15) Migrating to a new machine

1. Clone (shallow): 
   ```bash
   git clone --depth 1 --branch feature/streaming-mini-units https://github.com/Kyle-Riss/EchoStream.git
   cd EchoStream
   ```
2. Setup venv + requirements: 
   ```bash
   python3 -m venv .venv && source .venv/bin/activate
   pip install -r requirements.txt
   ```
3. Copy assets to the paths referenced in configs (or update the configs):
   - pretrain_models/ (mHuBERT, vocoder)
   - data/ (tsv manifests, units/*.npy, gcmvn.npz, src/tgt dicts)
4. Optional: Mini training run to adapt
5. Start server and open the web UI

16) Optional: Generating units

If you need to (re-)generate units for your target set:
- Use your mHuBERT model and K-means to produce text unit files, then convert per-utterance to .npy (we include a conversion snippet in prior discussions; integrate as needed).
- Update the *.units.tsv manifests to point to the produced .npy files in data/units/.

17) Contributing

- Create a feature branch and open a PR (we used feature/streaming-mini-units as a template branch).
- Avoid committing data/units/checkpoints; prefer release artifacts or internal storage.

18) License

Please refer to the repository license. External models (mHuBERT, CodeHiFiGAN, etc.) follow their respective licenses.

# EchoStream ğŸ¤â†’ğŸ—£ï¸

**EchoStream: Efficient Memory-based Streaming Speech-to-Speech Translation**

EchoStreamì€ Emformer ê¸°ë°˜ì˜ ê³ íš¨ìœ¨ ì‹¤ì‹œê°„ ìŒì„±-ìŒì„± ë²ˆì—­ ëª¨ë¸ì…ë‹ˆë‹¤. StreamSpeech ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜, Chunk-based Conformer ì¸ì½”ë”ë¥¼ **Emformer**ë¡œ êµì²´í•˜ì—¬ ê³„ì‚° íš¨ìœ¨ì„±ê³¼ ì²˜ë¦¬ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

---

## âœ¨ ì£¼ìš” íŠ¹ì§•

### ğŸš€ íš¨ìœ¨ì„± í–¥ìƒ
- **Left Context Cache**: ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ì˜ Key/Valueë¥¼ ìºì‹œí•˜ì—¬ ì¬ì‚¬ìš©
- **Augmented Memory Bank**: ì¥ê±°ë¦¬ ì˜ì¡´ì„±ì„ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ë§
- **ì—°ì‚° ë³µì¡ë„**: O(TÂ²) â†’ O(1) (ë°œí™” ê¸¸ì´ì™€ ë¬´ê´€í•˜ê²Œ ì¼ì •)

### âš¡ ì„±ëŠ¥ í–¥ìƒ
- **ì†ë„**: ê¸°ì¡´ ëŒ€ë¹„ 6-50ë°° ë¹ ë¦„ (ë°œí™” ê¸¸ì´ì— ë”°ë¼)
- **ë©”ëª¨ë¦¬**: 25ë°° ì ˆì•½
- **ì§€ì—° ì‹œê°„**: ì¼ì •í•œ ë‚®ì€ ì§€ì—° (ë°œí™” ê¸¸ì´ ë¬´ê´€)

### ğŸ¯ ì‹¤ì‹œê°„ ë²ˆì—­
- **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**: ì²­í¬ ë‹¨ìœ„ ì‹¤ì‹œê°„ ë²ˆì—­
- **CT-Transformer í†µí•©**: êµ¬ë‘ì  ê¸°ë°˜ ë¬¸ì¥ ê²½ê³„ íƒì§€ ë° ì¬ì¡°í•©
- **ë‚®ì€ ì§€ì—°**: 10ms ìˆ˜ì¤€ì˜ ì¸ì½”ë” ì§€ì—°

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              EchoStream Architecture                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Speech Input â†’ Emformer Encoder (16L)                â”‚
â”‚                    â†“                                 â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚        â†“                       â†“                    â”‚
â”‚  ASR CTC Decoder      ST CTC Decoder                â”‚
â”‚        â†“                       â†“                    â”‚
â”‚  CT-Transformer    MT Decoder (4L)                  â”‚
â”‚        â†“                       â†“                    â”‚
â”‚  Sentence Boundary   T2U Encoder (0L)               â”‚
â”‚        â†“                       â†“                    â”‚
â”‚  Recomposition    Unit Decoder (6L)                 â”‚
â”‚        â†“                       â†“                    â”‚
â”‚    Output â†â”€â”€â”€â”€ CodeHiFiGAN Vocoder                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸

1. **Emformer Encoder**: íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ì¸ì½”ë”
2. **CTC Decoders**: ASR ë° ST (Speech-to-Text) ì‘ì—…ìš©
3. **MT Decoder**: ê³ í’ˆì§ˆ í…ìŠ¤íŠ¸ ë²ˆì—­
4. **Unit Decoder**: í…ìŠ¤íŠ¸ë¥¼ ìŒì„± ìœ ë‹›ìœ¼ë¡œ ë³€í™˜
5. **CodeHiFiGAN**: ìœ ë‹›ì„ ê³ í’ˆì§ˆ ì˜¤ë””ì˜¤ë¡œ í•©ì„±
6. **CT-Transformer**: ì‹¤ì‹œê°„ êµ¬ë‘ì  ì˜ˆì¸¡ ë° ë¬¸ì¥ ê²½ê³„ íƒì§€

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

| ë©”íŠ¸ë¦­ | StreamSpeech (Conformer) | EchoStream (Emformer) | ê°œì„  |
|--------|-------------------------|----------------------|------|
| **ì¸ì½”ë” ì§€ì—°** (10ì´ˆ ë°œí™”) | ~60ms | ~10ms | **6ë°°** âš¡ |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** | ~256MB | ~10MB | **25ë°°** ğŸ’¾ |
| **ì—°ì‚° ë³µì¡ë„** | O(TÂ²) | O(1) | **ì¼ì •** ğŸš€ |
| **ì²˜ë¦¬ ì†ë„** | ë°œí™” ê¸¸ì´â†‘ â†’ ëŠë ¤ì§ | ë°œí™” ê¸¸ì´ ë¬´ê´€ | **ì¼ì •** âœ… |

---

## ğŸš€ ì‹œì‘í•˜ê¸°

### ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/Kyle-Riss/Ko-Speech2Speech.git
cd StreamSpeech

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### ë¹ ë¥¸ ì‹œì‘

```bash
# ì¶”ë¡  ì‹¤í–‰
python demo/infer.py \
    --model-path /path/to/model \
    --audio-path /path/to/audio.wav \
    --config configs/fr-en/config_unity.yaml
```

---

## ğŸ“š ë¬¸ì„œ

- [Emformer í†µí•© ê³„íš](EMFORMER_INTEGRATION_PLAN.md): Emformer ì¸ì½”ë” í†µí•© ìƒì„¸ ê³„íš
- [CT-Transformer í†µí•©](README_CT_TRANSFORMER_INTEGRATION.md): êµ¬ë‘ì  ì˜ˆì¸¡ ë° ì¬ì¡°í•© ì‹œìŠ¤í…œ
- [í•µì‹¬ íŒŒì¼ ê°€ì´ë“œ](CORE_FILES_FOR_REALTIME_TRANSLATION.md): ì‹¤ì‹œê°„ ë²ˆì—­ ê´€ë ¨ íŒŒì¼ ì •ë¦¬

---

## ğŸ”¬ ê¸°ë°˜ ì—°êµ¬

EchoStreamì€ ë‹¤ìŒ ì—°êµ¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤:

- **StreamSpeech**: [Streaming Speech-to-Speech Translation](https://arxiv.org/abs/2212.05758)
- **Emformer**: [Efficient Memory Transformer for Streaming ASR](https://arxiv.org/abs/2010.10759)
- **CT-Transformer**: [Controllable Time-Delay Transformer](https://ieeexplore.ieee.org/document/9054256)

---

## ğŸ“ ë¼ì´ì„ ìŠ¤

ë³¸ í”„ë¡œì íŠ¸ëŠ” ì›ë³¸ StreamSpeech ë° Fairseqì˜ ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

---

## ğŸ¤ ê¸°ì—¬

ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤! ì´ìŠˆì™€ í’€ ë¦¬í€˜ìŠ¤íŠ¸ë¥¼ í†µí•´ ì°¸ì—¬í•´ ì£¼ì„¸ìš”.

---

## ğŸ“§ ë¬¸ì˜

- Repository: [https://github.com/Kyle-Riss/Ko-Speech2Speech](https://github.com/Kyle-Riss/Ko-Speech2Speech)

---

**EchoStream** - ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ì‹¤ì‹œê°„ ìŒì„±-ìŒì„± ë²ˆì—­ ğŸŒŠ
