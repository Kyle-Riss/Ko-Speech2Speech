# EchoStream: Emformer Ïù∏ÏΩîÎçî ÌÜµÌï© Í≥ÑÌöç

**EchoStream**ÏùÄ StreamSpeechÏùò Chunk-based Conformer Ïù∏ÏΩîÎçîÎ•º EmformerÎ°ú ÍµêÏ≤¥ÌïòÏó¨ Ìö®Ïú®ÏÑ±Í≥º ÏÜçÎèÑÎ•º Ìñ•ÏÉÅÏãúÌÇ® Ïã§ÏãúÍ∞Ñ ÏùåÏÑ±-ÏùåÏÑ± Î≤àÏó≠ Î™®Îç∏ÏûÖÎãàÎã§.

## üéØ Î™©Ìëú

**StreamSpeech Î≤†Ïù¥Ïä§ÎùºÏù∏ÏóêÏÑú Ïù∏ÏΩîÎçîÎßå ÍµêÏ≤¥ÌïòÏó¨ Ìö®Ïú®ÏÑ± Ìñ•ÏÉÅ**

- ‚úÖ **Ïú†ÏßÄ**: StreamSpeechÏùò ÎîîÏΩîÎçî ÏïÑÌÇ§ÌÖçÏ≤ò (MT, Unit Decoder, CTC Ï†ïÏ±Ö)
- ‚úÖ **ÍµêÏ≤¥**: Streaming Speech Encoder (Conformer ‚Üí Emformer)
- ‚úÖ **Ìñ•ÏÉÅ**: Í≥ÑÏÇ∞ Ìö®Ïú®ÏÑ±, Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ, Ï≤òÎ¶¨ ÏÜçÎèÑ

---

## üìä ÌòÑÏû¨ vs Î™©Ìëú ÏïÑÌÇ§ÌÖçÏ≤ò

### ÌòÑÏû¨: StreamSpeech with Chunk-based Conformer

```mermaid
graph LR
    A[ÏùåÏÑ± ÏûÖÎ†•] --> B[Chunk-based Conformer<br/>Î¨∏Ï†ú: Í∏¥ Î∞úÌôî Ïãú Ïó∞ÏÇ∞Îüâ‚Üë]
    B --> C[CTC Decoders]
    B --> D[MT Decoder]
    D --> E[Unit Decoder]
    E --> F[CodeHiFiGAN]
    
    style B fill:#FFE66D
```

**Î¨∏Ï†úÏ†ê**:
```python
# Îß§ Ï≤≠ÌÅ¨ÎßàÎã§ 'Î™®Îì† Ïù¥Ï†Ñ Ï≤≠ÌÅ¨'Ïóê ÎåÄÌï¥ Ïñ¥ÌÖêÏÖò Í≥ÑÏÇ∞
for chunk_i in chunks:
    attention(chunk_i, all_previous_chunks)
    # Î∞úÌôî Í∏∏Ïù¥ ‚Üë ‚Üí Ïó∞ÏÇ∞Îüâ O(T¬≤) ‚Üë
```

### Î™©Ìëú: EchoStream (StreamSpeech + Emformer)

```mermaid
graph LR
    A[ÏùåÏÑ± ÏûÖÎ†•] --> B[Emformer Encoder<br/>Ìï¥Í≤∞: Ï∫êÏãú + Î©îÎ™®Î¶¨ Î±ÖÌÅ¨]
    B --> C[CTC Decoders<br/>ÎèôÏùº]
    B --> D[MT Decoder<br/>ÎèôÏùº]
    D --> E[Unit Decoder<br/>ÎèôÏùº]
    E --> F[CodeHiFiGAN<br/>ÎèôÏùº]
    
    style B fill:#4ECDC4
```

**Ìï¥Í≤∞Ï±Ö**:
```python
# Emformer: Ï∫êÏãú Ïû¨ÏÇ¨Ïö© + Î©îÎ™®Î¶¨ Î±ÖÌÅ¨
for segment_i in segments:
    # 1. Ï∫êÏãúÏóêÏÑú K, V Ïû¨ÏÇ¨Ïö©
    K_cache, V_cache = left_context_cache
    
    # 2. Î©îÎ™®Î¶¨ Î±ÖÌÅ¨ÏóêÏÑú Î®º Í≥ºÍ±∞ Ï∞∏Ï°∞
    M = memory_bank
    
    # 3. ÌòÑÏû¨ ÏÑ∏Í∑∏Î®ºÌä∏Îßå Q, K, V Í≥ÑÏÇ∞
    Q, K, V = compute(segment_i)
    
    attention(Q, [K_cache, K, K_memory])
    # Î∞úÌôî Í∏∏Ïù¥ÏôÄ Î¨¥Í¥ÄÌïòÍ≤å Ïó∞ÏÇ∞Îüâ ÏùºÏ†ï!
```

---

## üîç Emformer ÌïµÏã¨ Î©îÏª§ÎãàÏ¶ò

### 1. Left Context Cache (Ï¢åÏ∏° Î¨∏Îß• Ï∫êÏã±)

**ÌòÑÏû¨ StreamSpeech Î¨∏Ï†ú**:
```python
# Chunk i Ï≤òÎ¶¨ Ïãú
chunks = [c0, c1, c2, ..., c_i-1]  # Î™®Îì† Ïù¥Ï†Ñ Ï≤≠ÌÅ¨

# Îß§Î≤à Ï†ÑÏ≤¥ Í≥ÑÏÇ∞
for c in chunks:
    K, V = compute(c)  # Î∞òÎ≥µ Í≥ÑÏÇ∞!
    
attention(Q_i, K_all, V_all)
# Ïó∞ÏÇ∞: O(i √ó C √ó d) - iÍ∞ú Ï≤≠ÌÅ¨ Î™®Îëê Í≥ÑÏÇ∞
```

**Emformer Ìï¥Í≤∞**:
```python
# Ï¥àÍ∏∞Ìôî
K_cache = []
V_cache = []

# Segment i Ï≤òÎ¶¨ Ïãú
# 1. ÌòÑÏû¨ ÏÑ∏Í∑∏Î®ºÌä∏Îßå Í≥ÑÏÇ∞
Q_i, K_i, V_i = compute(segment_i)

# 2. Ï∫êÏãúÏóêÏÑú Ïù¥Ï†Ñ K, V Í∞ÄÏ†∏Ïò§Í∏∞
K_left = K_cache  # Ï†ÄÏû•Îêú Í≤É Ïû¨ÏÇ¨Ïö©!
V_left = V_cache

# 3. Attention
attention(Q_i, [K_left, K_i], [V_left, V_i])

# 4. Ï∫êÏãú ÏóÖÎç∞Ïù¥Ìä∏
K_cache.append(K_i)
V_cache.append(V_i)

# Ïó∞ÏÇ∞: O(C √ó d) - ÌòÑÏû¨ ÏÑ∏Í∑∏Î®ºÌä∏Îßå Í≥ÑÏÇ∞!
```

**Ìö®Ïú®ÏÑ± Ìñ•ÏÉÅ**:
```
Î∞úÌôî 10Ï¥à (100 Ï≤≠ÌÅ¨):
- StreamSpeech: 1 + 2 + 3 + ... + 100 = 5,050 Îã®ÏúÑ Ïó∞ÏÇ∞
- Emformer: 1 + 1 + 1 + ... + 1 = 100 Îã®ÏúÑ Ïó∞ÏÇ∞
‚Üí 50Î∞∞ Ìö®Ïú® Ìñ•ÏÉÅ!
```

### 2. Augmented Memory Bank (Ï¶ùÍ∞ï Î©îÎ™®Î¶¨ Î±ÖÌÅ¨)

**Í∞úÎÖê**: Î®º Í≥ºÍ±∞ Î¨∏Îß•ÏùÑ ÏïïÏ∂ïÌïòÏó¨ Ï†ÄÏû•

```python
# Memory Bank Íµ¨Ï°∞
M = [m1, m2, m3, ..., m_n]  # Í≥†Ï†ï ÌÅ¨Í∏∞ (Ïòà: n=8)

# ÏóÖÎç∞Ïù¥Ìä∏ Ï†ÑÎûµ
if len(segments) % S == 0:  # SÍ∞ú ÏÑ∏Í∑∏Î®ºÌä∏ÎßàÎã§
    # ÏöîÏïΩ Î≤°ÌÑ∞ ÏÉùÏÑ±
    summary = summarize(recent_segments)
    
    # Memory BankÏóê Ï∂îÍ∞Ä (FIFO)
    M.append(summary)
    if len(M) > n:
        M.pop(0)  # Í∞ÄÏû• Ïò§ÎûòÎêú Î©îÎ™®Î¶¨ Ï†úÍ±∞

# Attention Ïãú Memory Bank Ï∞∏Ï°∞
attention(Q_i, [K_cache, K_i, K_memory])
```

**Ïû•Ï†ê**:
- Ï†ÑÏ≤¥ Í≥ºÍ±∞Î•º Î≥¥ÏßÄ ÏïäÏïÑÎèÑ Ïû•Í±∞Î¶¨ ÏùòÏ°¥ÏÑ± Î™®Îç∏ÎßÅ
- Í≥†Ï†ïÎêú Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ
- Í∏¥ Î∞úÌôîÎèÑ Ìö®Ïú®Ï†Å Ï≤òÎ¶¨

### 3. Right Context (Ïö∞Ï∏° Î¨∏Îß•, ÏÑ†ÌÉùÏ†Å)

```python
# Look-ahead ÏòµÏÖò
right_context_size = R  # Ïòà: R=0 (Ïã§ÏãúÍ∞Ñ), R=3 (ÏïΩÍ∞ÑÏùò ÎØ∏Îûò)

# Attention Ïãú Ïö∞Ï∏° Î¨∏Îß•ÎèÑ Ï∞∏Ï°∞
if R > 0:
    attention(Q_i, [K_left, K_i, K_right, K_memory])
else:
    attention(Q_i, [K_left, K_i, K_memory])

# StreamSpeech: R=0 (ÏôÑÏ†Ñ Ïã§ÏãúÍ∞Ñ)
```

---

## üèóÔ∏è Íµ¨ÌòÑ Í≥ÑÌöç

### Phase 1: Emformer Î™®Îìà Íµ¨ÌòÑ

**ÌååÏùº**: `researches/ctc_unity/modules/emformer_layer.py`

```python
import torch
import torch.nn as nn
from typing import Optional, Tuple, List

class EmformerEncoderLayer(nn.Module):
    """
    Emformer Encoder Layer for StreamSpeech.
    
    Replaces Chunk-based Conformer with efficient memory transformer.
    """
    
    def __init__(
        self,
        embed_dim: int = 256,
        num_heads: int = 4,
        segment_length: int = 4,
        left_context_length: int = 30,
        right_context_length: int = 0,
        memory_size: int = 8,
        dropout: float = 0.1,
    ):
        super().__init__()
        
        # ÌååÎùºÎØ∏ÌÑ∞
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.segment_length = segment_length
        self.left_context_length = left_context_length
        self.right_context_length = right_context_length
        self.memory_size = memory_size
        
        # Multi-Head Attention
        self.self_attn = nn.MultiheadAttention(
            embed_dim, num_heads, dropout=dropout
        )
        
        # Feed-Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim * 4, embed_dim),
        )
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # Memory Bank Ï¥àÍ∏∞Ìôî
        self.memory_bank = None
        self.left_context_cache = {"K": [], "V": []}
    
    def forward(
        self,
        segment: torch.Tensor,
        left_context: Optional[torch.Tensor] = None,
        right_context: Optional[torch.Tensor] = None,
        memory_bank: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, dict]:
        """
        Forward pass with cached left context and memory bank.
        
        Args:
            segment: Current segment [T_seg, B, D]
            left_context: Cached left context [T_left, B, D]
            right_context: Right context [T_right, B, D] (optional)
            memory_bank: Memory bank [M, B, D] (optional)
        
        Returns:
            output: Processed segment [T_seg, B, D]
            cache: Updated cache for next segment
        """
        # ‚ë† Concatenate contexts
        contexts = [segment]
        if left_context is not None:
            contexts.insert(0, left_context)
        if right_context is not None:
            contexts.append(right_context)
        if memory_bank is not None:
            contexts.insert(0, memory_bank)
        
        full_context = torch.cat(contexts, dim=0)  # [T_total, B, D]
        
        # ‚ë° Self-Attention
        residual = segment
        segment = self.norm1(segment)
        
        # Query: ÌòÑÏû¨ ÏÑ∏Í∑∏Î®ºÌä∏Îßå
        # Key, Value: Ï†ÑÏ≤¥ Î¨∏Îß• (Ï∫êÏãú Ïû¨ÏÇ¨Ïö©)
        attn_out, _ = self.self_attn(
            query=segment,
            key=full_context,
            value=full_context,
        )
        
        segment = residual + self.dropout(attn_out)
        
        # ‚ë¢ Feed-Forward
        residual = segment
        segment = self.norm2(segment)
        segment = residual + self.dropout(self.ffn(segment))
        
        # ‚ë£ Cache ÏóÖÎç∞Ïù¥Ìä∏
        cache = {
            "output": segment,
            "K": full_context,  # Îã§Ïùå ÏÑ∏Í∑∏Î®ºÌä∏Ïö©
            "V": full_context,
        }
        
        return segment, cache
```

### Phase 2: Emformer Ïù∏ÏΩîÎçî Íµ¨ÌòÑ

**ÌååÏùº**: `researches/ctc_unity/models/emformer_encoder.py`

```python
class EmformerEncoder(nn.Module):
    """
    Emformer-based Speech Encoder for StreamSpeech.
    
    Replaces UniS2SConformerEncoder with efficient memory transformer.
    """
    
    def __init__(
        self,
        input_dim: int = 80,
        encoder_embed_dim: int = 256,
        num_layers: int = 16,
        num_heads: int = 4,
        segment_length: int = 4,
        left_context_length: int = 30,
        memory_size: int = 8,
    ):
        super().__init__()
        
        # Subsampling (Í∏∞Ï°¥Í≥º ÎèôÏùº)
        self.subsample = Conv2dSubsampler(
            input_channels=1,
            input_feat_per_channel=80,
            conv_out_channels=256,
            encoder_embed_dim=256,
        )
        
        # Emformer Layers
        self.layers = nn.ModuleList([
            EmformerEncoderLayer(
                embed_dim=encoder_embed_dim,
                num_heads=num_heads,
                segment_length=segment_length,
                left_context_length=left_context_length,
                memory_size=memory_size,
            )
            for _ in range(num_layers)
        ])
        
        # Cache Î∞è Memory Bank
        self.reset_cache()
    
    def reset_cache(self):
        """Reset cache for new utterance."""
        self.left_context_cache = []
        self.memory_bank = None
    
    def forward(self, src_tokens, src_lengths):
        """
        Forward with efficient caching.
        
        Args:
            src_tokens: [B, T, 80] Filter-bank features
            src_lengths: [B] Lengths
        
        Returns:
            encoder_out: Dict with output and cache
        """
        # ‚ë† Subsampling
        x, input_lengths = self.subsample(src_tokens, src_lengths)
        # x: [T', B, 256]
        
        # ‚ë° Segment into chunks
        T = x.size(0)
        S = self.layers[0].segment_length
        num_segments = (T + S - 1) // S
        
        outputs = []
        
        # ‚ë¢ Process each segment
        for seg_idx in range(num_segments):
            start = seg_idx * S
            end = min(start + S, T)
            segment = x[start:end]  # [S, B, 256]
            
            # Left context from cache
            left_context = self._get_left_context(seg_idx)
            
            # Process through layers
            for layer in self.layers:
                segment, cache = layer(
                    segment,
                    left_context=left_context,
                    memory_bank=self.memory_bank,
                )
                
                # Update cache
                self._update_cache(cache)
            
            outputs.append(segment)
        
        # ‚ë£ Concatenate outputs
        encoder_out = torch.cat(outputs, dim=0)  # [T', B, 256]
        
        return {
            "encoder_out": [encoder_out],
            "encoder_padding_mask": [],
            "encoder_embedding": [],
            "encoder_states": [],
            "src_tokens": [],
            "src_lengths": [],
        }
    
    def _get_left_context(self, seg_idx: int) -> Optional[torch.Tensor]:
        """Get left context from cache."""
        if seg_idx == 0 or not self.left_context_cache:
            return None
        
        # ÏµúÍ∑º LÍ∞ú ÏÑ∏Í∑∏Î®ºÌä∏
        L = self.layers[0].left_context_length
        start_idx = max(0, seg_idx - L)
        return torch.cat(self.left_context_cache[start_idx:seg_idx], dim=0)
    
    def _update_cache(self, cache: dict):
        """Update left context cache."""
        self.left_context_cache.append(cache["output"])
        
        # Ï∫êÏãú ÌÅ¨Í∏∞ Ï†úÌïú
        max_cache = self.layers[0].left_context_length + 10
        if len(self.left_context_cache) > max_cache:
            self.left_context_cache.pop(0)
```

---

## üìã Íµ¨ÌòÑ Îã®Í≥Ñ

### Step 1: Emformer Î†àÏù¥Ïñ¥ Íµ¨ÌòÑ ‚úì

**ÌååÏùº**: `researches/ctc_unity/modules/emformer_layer.py`

```python
class EmformerEncoderLayer(nn.Module):
    - Left Context Cache Î©îÏª§ÎãàÏ¶ò
    - Augmented Memory Bank
    - Efficient Attention
```

### Step 2: Emformer Ïù∏ÏΩîÎçî Íµ¨ÌòÑ

**ÌååÏùº**: `researches/ctc_unity/models/emformer_encoder.py`

```python
class EmformerSpeechEncoder(FairseqEncoder):
    - Subsampling (Í∏∞Ï°¥ Conv2d)
    - 16 Emformer Layers
    - Cache Management
```

### Step 3: StreamSpeech Î™®Îç∏Ïóê ÌÜµÌï©

**ÌååÏùº**: `researches/ctc_unity/models/streamspeech_emformer_model.py`

```python
@register_model("streamspeech_emformer")
class StreamSpeechEmformerModel(StreamSpeechModel):
    @classmethod
    def build_encoder(cls, args):
        # Emformer Ïù∏ÏΩîÎçî ÏÇ¨Ïö©
        return EmformerSpeechEncoder(args)
    
    # ÎÇòÎ®∏ÏßÄÎäî Í∏∞Ï°¥ StreamSpeechÏôÄ ÎèôÏùº
    # - CTC Decoders
    # - MT Decoder
    # - Unit Decoder
```

### Step 4: Agent ÏàòÏ†ï

**ÌååÏùº**: `agent/speech_to_speech.emformer.agent.py`

```python
class StreamSpeechEmformerAgent(StreamSpeechS2STAgent):
    def reset(self):
        super().reset()
        # Emformer Ï∫êÏãú Ï¥àÍ∏∞Ìôî
        for model in self.models:
            model.encoder.reset_cache()
```

---

## üî¨ ÏòàÏÉÅ Ìö®Í≥º

### 1. Í≥ÑÏÇ∞ Î≥µÏû°ÎèÑ ÎπÑÍµê

| Î∞úÌôî Í∏∏Ïù¥ | Chunk-based Conformer | Emformer | Ìñ•ÏÉÅ |
|----------|---------------------|----------|------|
| **1Ï¥à** (10 Ï≤≠ÌÅ¨) | O(55) | O(10) | **5.5Î∞∞** |
| **5Ï¥à** (50 Ï≤≠ÌÅ¨) | O(1,275) | O(50) | **25.5Î∞∞** |
| **10Ï¥à** (100 Ï≤≠ÌÅ¨) | O(5,050) | O(100) | **50.5Î∞∞** |
| **30Ï¥à** (300 Ï≤≠ÌÅ¨) | O(45,150) | O(300) | **150.5Î∞∞** |

### 2. Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ

```python
# StreamSpeech Conformer
memory = T √ó T √ó H √ó d  # Ïñ¥ÌÖêÏÖò Îßµ
# T=1000, H=4, d=64: ~256MB

# Emformer
memory = (S + L + M) √ó H √ó d  # ÏÑ∏Í∑∏Î®ºÌä∏ + Ï∫êÏãú + Î©îÎ™®Î¶¨
# S=4, L=30, M=8, H=4, d=64: ~10MB
‚Üí 25Î∞∞ Î©îÎ™®Î¶¨ Ï†àÏïΩ!
```

### 3. ÏßÄÏó∞ ÏãúÍ∞Ñ (Latency)

```python
# ÏÑ∏Í∑∏Î®ºÌä∏Îãπ Ï≤òÎ¶¨ ÏãúÍ∞Ñ
Conformer: 10ms + (Î∞úÌôî Í∏∏Ïù¥ √ó 0.5ms)
  - 1Ï¥à Î∞úÌôî: 15ms
  - 10Ï¥à Î∞úÌôî: 60ms ‚ùå

Emformer: 10ms (ÏùºÏ†ï)
  - 1Ï¥à Î∞úÌôî: 10ms
  - 10Ï¥à Î∞úÌôî: 10ms ‚úÖ

‚Üí Í∏¥ Î∞úÌôîÏóêÏÑú 6Î∞∞ Îπ†Î¶Ñ!
```

---

## üìä ÏïÑÌÇ§ÌÖçÏ≤ò ÎπÑÍµê

### Conformer Layer vs Emformer Layer

| Íµ¨ÏÑ± ÏöîÏÜå | Conformer | Emformer |
|----------|-----------|----------|
| **Self-Attention** | Î™®Îì† Ïù¥Ï†Ñ Ï≤≠ÌÅ¨ | Ï∫êÏãú + ÌòÑÏû¨ + Î©îÎ™®Î¶¨ |
| **Convolution** | Depthwise Conv | ÏóÜÏùå (ÎòêÎäî ÏÑ†ÌÉùÏ†Å) |
| **Feed-Forward** | 2048Ï∞®Ïõê | 2048Ï∞®Ïõê (ÎèôÏùº) |
| **Ïó∞ÏÇ∞Îüâ** | O(T¬≤) | O(1) |
| **Î©îÎ™®Î¶¨** | O(T¬≤) | O(1) |

### Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏ (Î≥ÄÍ≤Ω Ï†ÑÌõÑ)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          StreamSpeech (Baseline)                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Speech Encoder: Chunk-based Conformer            ‚îÇ ‚Üê ÍµêÏ≤¥ ÎåÄÏÉÅ
‚îÇ ASR CTC Decoder: CTCDecoder                      ‚îÇ ‚úì Ïú†ÏßÄ
‚îÇ ST CTC Decoder: CTCDecoderWithTransformerLayer   ‚îÇ ‚úì Ïú†ÏßÄ
‚îÇ MT Decoder: TransformerDecoder (4L)              ‚îÇ ‚úì Ïú†ÏßÄ
‚îÇ T2U Encoder: UniTransformerEncoderNoEmb (0L)    ‚îÇ ‚úì Ïú†ÏßÄ
‚îÇ Unit Decoder: CTCTransformerUnitDecoder (6L)    ‚îÇ ‚úì Ïú†ÏßÄ
‚îÇ Vocoder: CodeHiFiGAN                            ‚îÇ ‚úì Ïú†ÏßÄ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                    ‚Üì ÍµêÏ≤¥

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          EchoStream (Enhanced)                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Speech Encoder: Emformer (16L)                   ‚îÇ ‚≠ê ÏÉàÎ°úÏö¥
‚îÇ   - Left Context Cache                          ‚îÇ
‚îÇ   - Augmented Memory Bank                       ‚îÇ
‚îÇ   - Efficient Streaming Attention               ‚îÇ
‚îÇ ASR CTC Decoder: CTCDecoder                      ‚îÇ ‚úì ÎèôÏùº
‚îÇ ST CTC Decoder: CTCDecoderWithTransformerLayer   ‚îÇ ‚úì ÎèôÏùº
‚îÇ MT Decoder: TransformerDecoder (4L)              ‚îÇ ‚úì ÎèôÏùº
‚îÇ T2U Encoder: UniTransformerEncoderNoEmb (0L)    ‚îÇ ‚úì ÎèôÏùº
‚îÇ Unit Decoder: CTCTransformerUnitDecoder (6L)    ‚îÇ ‚úì ÎèôÏùº
‚îÇ Vocoder: CodeHiFiGAN                            ‚îÇ ‚úì ÎèôÏùº
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéØ EchoStream ÌÜµÌï© Ìè¨Ïù∏Ìä∏

### ÏΩîÎìú ÏàòÏ†ï ÏúÑÏπò

**1. Î™®Îç∏ Ï†ïÏùò**: `researches/ctc_unity/models/echostream_model.py`

```python
# Í∏∞Ï°¥
from researches.ctc_unity.models.s2s_conformer import UniS2SConformerEncoder

# Î≥ÄÍ≤Ω
from researches.ctc_unity.models.emformer_encoder import EmformerSpeechEncoder

@register_model("echostream")
class EchoStreamModel(StreamSpeechModel):
    @classmethod
    def build_encoder(cls, args):
        # Conformer ‚Üí Emformer ÍµêÏ≤¥
        return EmformerSpeechEncoder(args)
```

**2. Agent**: `agent/speech_to_speech.echostream.agent.py`

```python
# Ï∫êÏãú Í¥ÄÎ¶¨ Ï∂îÍ∞Ä
def reset(self):
    super().reset()
    for model in self.models:
        if hasattr(model.encoder, 'reset_cache'):
            model.encoder.reset_cache()
```

**3. ÏÑ§Ï†ï**: `configs/fr-en/config_echostream.yaml`

```yaml
# EchoStream ÌååÎùºÎØ∏ÌÑ∞
model_name: echostream
encoder_type: emformer
segment_length: 4          # 40ms @ 100fps
left_context_length: 30    # 300ms
right_context_length: 0    # ÏôÑÏ†Ñ Ïã§ÏãúÍ∞Ñ
memory_size: 8             # Î©îÎ™®Î¶¨ Î±ÖÌÅ¨ ÌÅ¨Í∏∞
```

---

## üìà ÏÑ±Îä• ÏòàÏ∏°

### Í∏∞Ï°¥ StreamSpeech (Conformer)

```
Î∞úÌôî 10Ï¥à:
  - Ïù∏ÏΩîÎçî ÏßÄÏó∞: ~60ms
  - Î©îÎ™®Î¶¨: ~256MB
  - Ïó∞ÏÇ∞Îüâ: O(T¬≤)
```

### EchoStream (StreamSpeech + Emformer)

```
Î∞úÌôî 10Ï¥à:
  - Ïù∏ÏΩîÎçî ÏßÄÏó∞: ~10ms ‚ö° (6Î∞∞ Îπ†Î¶Ñ)
  - Î©îÎ™®Î¶¨: ~10MB üíæ (25Î∞∞ Ï†àÏïΩ)
  - Ïó∞ÏÇ∞Îüâ: O(1) üöÄ (ÏùºÏ†ï)
```

### ÌíàÏßà Ïú†ÏßÄ

- ‚úÖ ÎèôÏùºÌïú Transformer Í∏∞Î∞ò
- ‚úÖ ÎèôÏùºÌïú Multi-Head Attention
- ‚úÖ Î©îÎ™®Î¶¨ Î±ÖÌÅ¨Î°ú Ïû•Í±∞Î¶¨ ÏùòÏ°¥ÏÑ± Ïú†ÏßÄ
- ‚úÖ Í∏∞Ï°¥ ÎîîÏΩîÎçî Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©

---

## üîÑ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò Ï†ÑÎûµ

### Îã®Í≥ÑÎ≥Ñ Ï†ÅÏö©

```
Phase 1: Íµ¨ÌòÑ (1-2Ï£º)
  ‚îú‚îÄ EmformerEncoderLayer Íµ¨ÌòÑ
  ‚îú‚îÄ EmformerSpeechEncoder Íµ¨ÌòÑ
  ‚îî‚îÄ Îã®ÏúÑ ÌÖåÏä§Ìä∏

Phase 2: ÌÜµÌï© (1Ï£º)
  ‚îú‚îÄ EchoStreamModel ÏÉùÏÑ±
  ‚îú‚îÄ Agent ÏàòÏ†ï
  ‚îî‚îÄ ÌÜµÌï© ÌÖåÏä§Ìä∏

Phase 3: ÌïôÏäµ (2-4Ï£º)
  ‚îú‚îÄ Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞Î°ú Ïû¨ÌïôÏäµ
  ‚îú‚îÄ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù
  ‚îî‚îÄ ÏÑ±Îä• Í≤ÄÏ¶ù

Phase 4: ÌèâÍ∞Ä (1Ï£º)
  ‚îú‚îÄ BLEU, ASR-BLEU
  ‚îú‚îÄ Latency (AL, AP, DAL)
  ‚îî‚îÄ ÌíàÏßà ÎπÑÍµê
```

---

## üí° Í≤∞Î°†

### ‚úÖ Í∞ÄÎä•ÏÑ±

**ÏôÑÏ†ÑÌûà Í∞ÄÎä•Ìï©ÎãàÎã§!** Ïù¥Ïú†:

1. **Î™®ÎìàÏãù ÏÑ§Í≥Ñ**: StreamSpeechÎäî Ïù∏ÏΩîÎçî/ÎîîÏΩîÎçîÍ∞Ä ÎèÖÎ¶ΩÏ†Å
2. **ÎèôÏùºÌïú Ï∂úÎ†• ÌòïÏãù**: Îëò Îã§ `[T, B, 256]` Ï∂úÎ†•
3. **Í≤ÄÏ¶ùÎêú Í∏∞Ïà†**: EmformerÎäî Ïù¥ÎØ∏ fairseqÏóê Íµ¨ÌòÑÎê®
4. **Ìò∏ÌôòÏÑ±**: ÎÇòÎ®∏ÏßÄ Ïª¥Ìè¨ÎÑåÌä∏ ÏàòÏ†ï Î∂àÌïÑÏöî

### üìà ÏòàÏÉÅ Ïù¥Ï†ê

| Î©îÌä∏Î¶≠ | Í∞úÏÑ† |
|--------|------|
| **ÏÜçÎèÑ** | ‚¨ÜÔ∏è 6-50Î∞∞ (Î∞úÌôî Í∏∏Ïù¥Ïóê Îî∞Îùº) |
| **Î©îÎ™®Î¶¨** | ‚¨áÔ∏è 25Î∞∞ Ï†àÏïΩ |
| **ÏßÄÏó∞** | ‚¨áÔ∏è ÏùºÏ†ï (Î∞úÌôî Í∏∏Ïù¥ Î¨¥Í¥Ä) |
| **ÌíàÏßà** | ‚û°Ô∏è Ïú†ÏßÄ ÎòêÎäî ÏÜåÌè≠ Ìñ•ÏÉÅ |
| **ÌôïÏû•ÏÑ±** | ‚¨ÜÔ∏è Í∏¥ Î∞úÌôî Ï≤òÎ¶¨ Í∞ÄÎä• |

### üéØ ÌïµÏã¨ Ïù∏ÏÇ¨Ïù¥Ìä∏

**EchoStreamÏùò ÌïµÏã¨**: EmformerÏùò Left Context Cache + Memory BankÎäî StreamSpeechÏùò Chunk-based ConformerÎ•º ÏôÑÎ≤ΩÌïòÍ≤å ÎåÄÏ≤¥Ìï† Ïàò ÏûàÏúºÎ©∞, Ìõ®Ïî¨ Ìö®Ïú®Ï†ÅÏûÖÎãàÎã§!

---

## üöÄ Îã§Ïùå Îã®Í≥Ñ

1. ‚úÖ ÌîÑÎ°úÏ†ùÌä∏ Ïù¥Î¶Ñ ÌôïÏ†ï: **EchoStream**
2. ‚úÖ README ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å
3. ‚è≠Ô∏è Emformer Î™®Îìà Íµ¨ÌòÑ ÏãúÏûë
4. ‚è≠Ô∏è ÌÜµÌï© Î∞è ÌÖåÏä§Ìä∏
5. ‚è≠Ô∏è ÌïôÏäµ Î∞è ÌèâÍ∞Ä

**EchoStream Í∞úÎ∞úÏùÑ ÏãúÏûëÌï©ÎãàÎã§!** üéä


