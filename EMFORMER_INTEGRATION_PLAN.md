# EchoStream: Emformer ì¸ì½”ë” í†µí•© ê³„íš

**EchoStream**ì€ StreamSpeechì˜ Chunk-based Conformer ì¸ì½”ë”ë¥¼ Emformerë¡œ êµì²´í•˜ì—¬ íš¨ìœ¨ì„±ê³¼ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¨ ì‹¤ì‹œê°„ ìŒì„±-ìŒì„± ë²ˆì—­ ëª¨ë¸ì…ë‹ˆë‹¤.

## ğŸ¯ ëª©í‘œ

**StreamSpeech ë² ì´ìŠ¤ë¼ì¸ì—ì„œ ì¸ì½”ë”ë§Œ êµì²´í•˜ì—¬ íš¨ìœ¨ì„± í–¥ìƒ**

- âœ… **ìœ ì§€**: StreamSpeechì˜ ë””ì½”ë” ì•„í‚¤í…ì²˜ (MT, Unit Decoder, CTC ì •ì±…)
- âœ… **êµì²´**: Streaming Speech Encoder (Conformer â†’ Emformer)
- âœ… **í–¥ìƒ**: ê³„ì‚° íš¨ìœ¨ì„±, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ì²˜ë¦¬ ì†ë„

---

## ğŸ“Š í˜„ì¬ vs ëª©í‘œ ì•„í‚¤í…ì²˜

### í˜„ì¬: StreamSpeech with Chunk-based Conformer

```mermaid
graph LR
    A[ìŒì„± ì…ë ¥] --> B[Chunk-based Conformer<br/>ë¬¸ì œ: ê¸´ ë°œí™” ì‹œ ì—°ì‚°ëŸ‰â†‘]
    B --> C[CTC Decoders]
    B --> D[MT Decoder]
    D --> E[Unit Decoder]
    E --> F[CodeHiFiGAN]
    
    style B fill:#FFE66D
```

**ë¬¸ì œì **:
```python
# ë§¤ ì²­í¬ë§ˆë‹¤ 'ëª¨ë“  ì´ì „ ì²­í¬'ì— ëŒ€í•´ ì–´í…ì…˜ ê³„ì‚°
for chunk_i in chunks:
    attention(chunk_i, all_previous_chunks)
    # ë°œí™” ê¸¸ì´ â†‘ â†’ ì—°ì‚°ëŸ‰ O(TÂ²) â†‘
```

### ëª©í‘œ: EchoStream (StreamSpeech + Emformer)

```mermaid
graph LR
    A[ìŒì„± ì…ë ¥] --> B[Emformer Encoder<br/>í•´ê²°: ìºì‹œ + ë©”ëª¨ë¦¬ ë±…í¬]
    B --> C[CTC Decoders<br/>ë™ì¼]
    B --> D[MT Decoder<br/>ë™ì¼]
    D --> E[Unit Decoder<br/>ë™ì¼]
    E --> F[CodeHiFiGAN<br/>ë™ì¼]
    
    style B fill:#4ECDC4
```

**í•´ê²°ì±…**:
```python
# Emformer: ìºì‹œ ì¬ì‚¬ìš© + ë©”ëª¨ë¦¬ ë±…í¬
for segment_i in segments:
    # 1. ìºì‹œì—ì„œ K, V ì¬ì‚¬ìš©
    K_cache, V_cache = left_context_cache
    
    # 2. ë©”ëª¨ë¦¬ ë±…í¬ì—ì„œ ë¨¼ ê³¼ê±° ì°¸ì¡°
    M = memory_bank
    
    # 3. í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ë§Œ Q, K, V ê³„ì‚°
    Q, K, V = compute(segment_i)
    
    attention(Q, [K_cache, K, K_memory])
    # ë°œí™” ê¸¸ì´ì™€ ë¬´ê´€í•˜ê²Œ ì—°ì‚°ëŸ‰ ì¼ì •!
```

---

## ğŸ” Emformer í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜

### 1. Left Context Cache (ì¢Œì¸¡ ë¬¸ë§¥ ìºì‹±)

**í˜„ì¬ StreamSpeech ë¬¸ì œ**:
```python
# Chunk i ì²˜ë¦¬ ì‹œ
chunks = [c0, c1, c2, ..., c_i-1]  # ëª¨ë“  ì´ì „ ì²­í¬

# ë§¤ë²ˆ ì „ì²´ ê³„ì‚°
for c in chunks:
    K, V = compute(c)  # ë°˜ë³µ ê³„ì‚°!
    
attention(Q_i, K_all, V_all)
# ì—°ì‚°: O(i Ã— C Ã— d) - iê°œ ì²­í¬ ëª¨ë‘ ê³„ì‚°
```

**Emformer í•´ê²°**:
```python
# ì´ˆê¸°í™”
K_cache = []
V_cache = []

# Segment i ì²˜ë¦¬ ì‹œ
# 1. í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ë§Œ ê³„ì‚°
Q_i, K_i, V_i = compute(segment_i)

# 2. ìºì‹œì—ì„œ ì´ì „ K, V ê°€ì ¸ì˜¤ê¸°
K_left = K_cache  # ì €ì¥ëœ ê²ƒ ì¬ì‚¬ìš©!
V_left = V_cache

# 3. Attention
attention(Q_i, [K_left, K_i], [V_left, V_i])

# 4. ìºì‹œ ì—…ë°ì´íŠ¸
K_cache.append(K_i)
V_cache.append(V_i)

# ì—°ì‚°: O(C Ã— d) - í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ë§Œ ê³„ì‚°!
```

**íš¨ìœ¨ì„± í–¥ìƒ**:
```
ë°œí™” 10ì´ˆ (100 ì²­í¬):
- StreamSpeech: 1 + 2 + 3 + ... + 100 = 5,050 ë‹¨ìœ„ ì—°ì‚°
- Emformer: 1 + 1 + 1 + ... + 1 = 100 ë‹¨ìœ„ ì—°ì‚°
â†’ 50ë°° íš¨ìœ¨ í–¥ìƒ!
```

### 2. Augmented Memory Bank (ì¦ê°• ë©”ëª¨ë¦¬ ë±…í¬)

**ê°œë…**: ë¨¼ ê³¼ê±° ë¬¸ë§¥ì„ ì••ì¶•í•˜ì—¬ ì €ì¥

```python
# Memory Bank êµ¬ì¡°
M = [m1, m2, m3, ..., m_n]  # ê³ ì • í¬ê¸° (ì˜ˆ: n=8)

# ì—…ë°ì´íŠ¸ ì „ëµ
if len(segments) % S == 0:  # Sê°œ ì„¸ê·¸ë¨¼íŠ¸ë§ˆë‹¤
    # ìš”ì•½ ë²¡í„° ìƒì„±
    summary = summarize(recent_segments)
    
    # Memory Bankì— ì¶”ê°€ (FIFO)
    M.append(summary)
    if len(M) > n:
        M.pop(0)  # ê°€ì¥ ì˜¤ë˜ëœ ë©”ëª¨ë¦¬ ì œê±°

# Attention ì‹œ Memory Bank ì°¸ì¡°
attention(Q_i, [K_cache, K_i, K_memory])
```

**ì¥ì **:
- ì „ì²´ ê³¼ê±°ë¥¼ ë³´ì§€ ì•Šì•„ë„ ì¥ê±°ë¦¬ ì˜ì¡´ì„± ëª¨ë¸ë§
- ê³ ì •ëœ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- ê¸´ ë°œí™”ë„ íš¨ìœ¨ì  ì²˜ë¦¬

### 3. Right Context (ìš°ì¸¡ ë¬¸ë§¥, ì„ íƒì )

```python
# Look-ahead ì˜µì…˜
right_context_size = R  # ì˜ˆ: R=0 (ì‹¤ì‹œê°„), R=3 (ì•½ê°„ì˜ ë¯¸ë˜)

# Attention ì‹œ ìš°ì¸¡ ë¬¸ë§¥ë„ ì°¸ì¡°
if R > 0:
    attention(Q_i, [K_left, K_i, K_right, K_memory])
else:
    attention(Q_i, [K_left, K_i, K_memory])

# StreamSpeech: R=0 (ì™„ì „ ì‹¤ì‹œê°„)
```

---

## ğŸ—ï¸ êµ¬í˜„ ê³„íš

### Phase 1: Emformer ëª¨ë“ˆ êµ¬í˜„

**íŒŒì¼**: `researches/ctc_unity/modules/emformer_layer.py`

```python
import torch
import torch.nn as nn
from typing import Optional, Tuple, List

class EmformerEncoderLayer(nn.Module):
    """
    Emformer Encoder Layer for StreamSpeech.
    
    Replaces Chunk-based Conformer with efficient memory transformer.
    """
    
    def __init__(
        self,
        embed_dim: int = 256,
        num_heads: int = 4,
        segment_length: int = 4,
        left_context_length: int = 30,
        right_context_length: int = 0,
        memory_size: int = 8,
        dropout: float = 0.1,
    ):
        super().__init__()
        
        # íŒŒë¼ë¯¸í„°
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.segment_length = segment_length
        self.left_context_length = left_context_length
        self.right_context_length = right_context_length
        self.memory_size = memory_size
        
        # Multi-Head Attention
        self.self_attn = nn.MultiheadAttention(
            embed_dim, num_heads, dropout=dropout
        )
        
        # Feed-Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim * 4, embed_dim),
        )
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # Memory Bank ì´ˆê¸°í™”
        self.memory_bank = None
        self.left_context_cache = {"K": [], "V": []}
    
    def forward(
        self,
        segment: torch.Tensor,
        left_context: Optional[torch.Tensor] = None,
        right_context: Optional[torch.Tensor] = None,
        memory_bank: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, dict]:
        """
        Forward pass with cached left context and memory bank.
        
        Args:
            segment: Current segment [T_seg, B, D]
            left_context: Cached left context [T_left, B, D]
            right_context: Right context [T_right, B, D] (optional)
            memory_bank: Memory bank [M, B, D] (optional)
        
        Returns:
            output: Processed segment [T_seg, B, D]
            cache: Updated cache for next segment
        """
        # â‘  Concatenate contexts
        contexts = [segment]
        if left_context is not None:
            contexts.insert(0, left_context)
        if right_context is not None:
            contexts.append(right_context)
        if memory_bank is not None:
            contexts.insert(0, memory_bank)
        
        full_context = torch.cat(contexts, dim=0)  # [T_total, B, D]
        
        # â‘¡ Self-Attention
        residual = segment
        segment = self.norm1(segment)
        
        # Query: í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ë§Œ
        # Key, Value: ì „ì²´ ë¬¸ë§¥ (ìºì‹œ ì¬ì‚¬ìš©)
        attn_out, _ = self.self_attn(
            query=segment,
            key=full_context,
            value=full_context,
        )
        
        segment = residual + self.dropout(attn_out)
        
        # â‘¢ Feed-Forward
        residual = segment
        segment = self.norm2(segment)
        segment = residual + self.dropout(self.ffn(segment))
        
        # â‘£ Cache ì—…ë°ì´íŠ¸
        cache = {
            "output": segment,
            "K": full_context,  # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ìš©
            "V": full_context,
        }
        
        return segment, cache
```

### Phase 2: Emformer ì¸ì½”ë” êµ¬í˜„

**íŒŒì¼**: `researches/ctc_unity/models/emformer_encoder.py`

```python
class EmformerEncoder(nn.Module):
    """
    Emformer-based Speech Encoder for StreamSpeech.
    
    Replaces UniS2SConformerEncoder with efficient memory transformer.
    """
    
    def __init__(
        self,
        input_dim: int = 80,
        encoder_embed_dim: int = 256,
        num_layers: int = 16,
        num_heads: int = 4,
        segment_length: int = 4,
        left_context_length: int = 30,
        memory_size: int = 8,
    ):
        super().__init__()
        
        # Subsampling (ê¸°ì¡´ê³¼ ë™ì¼)
        self.subsample = Conv2dSubsampler(
            input_channels=1,
            input_feat_per_channel=80,
            conv_out_channels=256,
            encoder_embed_dim=256,
        )
        
        # Emformer Layers
        self.layers = nn.ModuleList([
            EmformerEncoderLayer(
                embed_dim=encoder_embed_dim,
                num_heads=num_heads,
                segment_length=segment_length,
                left_context_length=left_context_length,
                memory_size=memory_size,
            )
            for _ in range(num_layers)
        ])
        
        # Cache ë° Memory Bank
        self.reset_cache()
    
    def reset_cache(self):
        """Reset cache for new utterance."""
        self.left_context_cache = []
        self.memory_bank = None
    
    def forward(self, src_tokens, src_lengths):
        """
        Forward with efficient caching.
        
        Args:
            src_tokens: [B, T, 80] Filter-bank features
            src_lengths: [B] Lengths
        
        Returns:
            encoder_out: Dict with output and cache
        """
        # â‘  Subsampling
        x, input_lengths = self.subsample(src_tokens, src_lengths)
        # x: [T', B, 256]
        
        # â‘¡ Segment into chunks
        T = x.size(0)
        S = self.layers[0].segment_length
        num_segments = (T + S - 1) // S
        
        outputs = []
        
        # â‘¢ Process each segment
        for seg_idx in range(num_segments):
            start = seg_idx * S
            end = min(start + S, T)
            segment = x[start:end]  # [S, B, 256]
            
            # Left context from cache
            left_context = self._get_left_context(seg_idx)
            
            # Process through layers
            for layer in self.layers:
                segment, cache = layer(
                    segment,
                    left_context=left_context,
                    memory_bank=self.memory_bank,
                )
                
                # Update cache
                self._update_cache(cache)
            
            outputs.append(segment)
        
        # â‘£ Concatenate outputs
        encoder_out = torch.cat(outputs, dim=0)  # [T', B, 256]
        
        return {
            "encoder_out": [encoder_out],
            "encoder_padding_mask": [],
            "encoder_embedding": [],
            "encoder_states": [],
            "src_tokens": [],
            "src_lengths": [],
        }
    
    def _get_left_context(self, seg_idx: int) -> Optional[torch.Tensor]:
        """Get left context from cache."""
        if seg_idx == 0 or not self.left_context_cache:
            return None
        
        # ìµœê·¼ Lê°œ ì„¸ê·¸ë¨¼íŠ¸
        L = self.layers[0].left_context_length
        start_idx = max(0, seg_idx - L)
        return torch.cat(self.left_context_cache[start_idx:seg_idx], dim=0)
    
    def _update_cache(self, cache: dict):
        """Update left context cache."""
        self.left_context_cache.append(cache["output"])
        
        # ìºì‹œ í¬ê¸° ì œí•œ
        max_cache = self.layers[0].left_context_length + 10
        if len(self.left_context_cache) > max_cache:
            self.left_context_cache.pop(0)
```

---

## ğŸ“‹ êµ¬í˜„ ë‹¨ê³„

### Step 1: Emformer ë ˆì´ì–´ êµ¬í˜„ âœ“

**íŒŒì¼**: `researches/ctc_unity/modules/emformer_layer.py`

```python
class EmformerEncoderLayer(nn.Module):
    - Left Context Cache ë©”ì»¤ë‹ˆì¦˜
    - Augmented Memory Bank
    - Efficient Attention
```

### Step 2: Emformer ì¸ì½”ë” êµ¬í˜„

**íŒŒì¼**: `researches/ctc_unity/models/emformer_encoder.py`

```python
class EmformerSpeechEncoder(FairseqEncoder):
    - Subsampling (ê¸°ì¡´ Conv2d)
    - 16 Emformer Layers
    - Cache Management
```

### Step 3: StreamSpeech ëª¨ë¸ì— í†µí•©

**íŒŒì¼**: `researches/ctc_unity/models/streamspeech_emformer_model.py`

```python
@register_model("streamspeech_emformer")
class StreamSpeechEmformerModel(StreamSpeechModel):
    @classmethod
    def build_encoder(cls, args):
        # Emformer ì¸ì½”ë” ì‚¬ìš©
        return EmformerSpeechEncoder(args)
    
    # ë‚˜ë¨¸ì§€ëŠ” ê¸°ì¡´ StreamSpeechì™€ ë™ì¼
    # - CTC Decoders
    # - MT Decoder
    # - Unit Decoder
```

### Step 4: Agent ìˆ˜ì •

**íŒŒì¼**: `agent/speech_to_speech.emformer.agent.py`

```python
class StreamSpeechEmformerAgent(StreamSpeechS2STAgent):
    def reset(self):
        super().reset()
        # Emformer ìºì‹œ ì´ˆê¸°í™”
        for model in self.models:
            model.encoder.reset_cache()
```

---

## ğŸ”¬ ì˜ˆìƒ íš¨ê³¼

### 1. ê³„ì‚° ë³µì¡ë„ ë¹„êµ

| ë°œí™” ê¸¸ì´ | Chunk-based Conformer | Emformer | í–¥ìƒ |
|----------|---------------------|----------|------|
| **1ì´ˆ** (10 ì²­í¬) | O(55) | O(10) | **5.5ë°°** |
| **5ì´ˆ** (50 ì²­í¬) | O(1,275) | O(50) | **25.5ë°°** |
| **10ì´ˆ** (100 ì²­í¬) | O(5,050) | O(100) | **50.5ë°°** |
| **30ì´ˆ** (300 ì²­í¬) | O(45,150) | O(300) | **150.5ë°°** |

### 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

```python
# StreamSpeech Conformer
memory = T Ã— T Ã— H Ã— d  # ì–´í…ì…˜ ë§µ
# T=1000, H=4, d=64: ~256MB

# Emformer
memory = (S + L + M) Ã— H Ã— d  # ì„¸ê·¸ë¨¼íŠ¸ + ìºì‹œ + ë©”ëª¨ë¦¬
# S=4, L=30, M=8, H=4, d=64: ~10MB
â†’ 25ë°° ë©”ëª¨ë¦¬ ì ˆì•½!
```

### 3. ì§€ì—° ì‹œê°„ (Latency)

```python
# ì„¸ê·¸ë¨¼íŠ¸ë‹¹ ì²˜ë¦¬ ì‹œê°„
Conformer: 10ms + (ë°œí™” ê¸¸ì´ Ã— 0.5ms)
  - 1ì´ˆ ë°œí™”: 15ms
  - 10ì´ˆ ë°œí™”: 60ms âŒ

Emformer: 10ms (ì¼ì •)
  - 1ì´ˆ ë°œí™”: 10ms
  - 10ì´ˆ ë°œí™”: 10ms âœ…

â†’ ê¸´ ë°œí™”ì—ì„œ 6ë°° ë¹ ë¦„!
```

---

## ğŸ“Š ì•„í‚¤í…ì²˜ ë¹„êµ

### Conformer Layer vs Emformer Layer

| êµ¬ì„± ìš”ì†Œ | Conformer | Emformer |
|----------|-----------|----------|
| **Self-Attention** | ëª¨ë“  ì´ì „ ì²­í¬ | ìºì‹œ + í˜„ì¬ + ë©”ëª¨ë¦¬ |
| **Convolution** | Depthwise Conv | ì—†ìŒ (ë˜ëŠ” ì„ íƒì ) |
| **Feed-Forward** | 2048ì°¨ì› | 2048ì°¨ì› (ë™ì¼) |
| **ì—°ì‚°ëŸ‰** | O(TÂ²) | O(1) |
| **ë©”ëª¨ë¦¬** | O(TÂ²) | O(1) |

### ì „ì²´ íŒŒì´í”„ë¼ì¸ (ë³€ê²½ ì „í›„)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          StreamSpeech (Baseline)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Speech Encoder: Chunk-based Conformer            â”‚ â† êµì²´ ëŒ€ìƒ
â”‚ ASR CTC Decoder: CTCDecoder                      â”‚ âœ“ ìœ ì§€
â”‚ ST CTC Decoder: CTCDecoderWithTransformerLayer   â”‚ âœ“ ìœ ì§€
â”‚ MT Decoder: TransformerDecoder (4L)              â”‚ âœ“ ìœ ì§€
â”‚ T2U Encoder: UniTransformerEncoderNoEmb (0L)    â”‚ âœ“ ìœ ì§€
â”‚ Unit Decoder: CTCTransformerUnitDecoder (6L)    â”‚ âœ“ ìœ ì§€
â”‚ Vocoder: CodeHiFiGAN                            â”‚ âœ“ ìœ ì§€
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â†“ êµì²´

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          EchoStream (Enhanced)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Speech Encoder: Emformer (16L)                   â”‚ â­ ìƒˆë¡œìš´
â”‚   - Left Context Cache                          â”‚
â”‚   - Augmented Memory Bank                       â”‚
â”‚   - Efficient Streaming Attention               â”‚
â”‚ ASR CTC Decoder: CTCDecoder                      â”‚ âœ“ ë™ì¼
â”‚ ST CTC Decoder: CTCDecoderWithTransformerLayer   â”‚ âœ“ ë™ì¼
â”‚ MT Decoder: TransformerDecoder (4L)              â”‚ âœ“ ë™ì¼
â”‚ T2U Encoder: UniTransformerEncoderNoEmb (0L)    â”‚ âœ“ ë™ì¼
â”‚ Unit Decoder: CTCTransformerUnitDecoder (6L)    â”‚ âœ“ ë™ì¼
â”‚ Vocoder: CodeHiFiGAN                            â”‚ âœ“ ë™ì¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ EchoStream í†µí•© í¬ì¸íŠ¸

### ì½”ë“œ ìˆ˜ì • ìœ„ì¹˜

**1. ëª¨ë¸ ì •ì˜**: `researches/ctc_unity/models/echostream_model.py`

```python
# ê¸°ì¡´
from researches.ctc_unity.models.s2s_conformer import UniS2SConformerEncoder

# ë³€ê²½
from researches.ctc_unity.models.emformer_encoder import EmformerSpeechEncoder

@register_model("echostream")
class EchoStreamModel(StreamSpeechModel):
    @classmethod
    def build_encoder(cls, args):
        # Conformer â†’ Emformer êµì²´
        return EmformerSpeechEncoder(args)
```

**2. Agent**: `agent/speech_to_speech.echostream.agent.py`

```python
# ìºì‹œ ê´€ë¦¬ ì¶”ê°€
def reset(self):
    super().reset()
    for model in self.models:
        if hasattr(model.encoder, 'reset_cache'):
            model.encoder.reset_cache()
```

**3. ì„¤ì •**: `configs/fr-en/config_echostream.yaml`

```yaml
# EchoStream íŒŒë¼ë¯¸í„°
model_name: echostream
encoder_type: emformer
segment_length: 4          # 40ms @ 100fps
left_context_length: 30    # 300ms
right_context_length: 0    # ì™„ì „ ì‹¤ì‹œê°„
memory_size: 8             # ë©”ëª¨ë¦¬ ë±…í¬ í¬ê¸°
```

---

## ğŸ“ˆ ì„±ëŠ¥ ì˜ˆì¸¡

### ê¸°ì¡´ StreamSpeech (Conformer)

```
ë°œí™” 10ì´ˆ:
  - ì¸ì½”ë” ì§€ì—°: ~60ms
  - ë©”ëª¨ë¦¬: ~256MB
  - ì—°ì‚°ëŸ‰: O(TÂ²)
```

### EchoStream (StreamSpeech + Emformer)

```
ë°œí™” 10ì´ˆ:
  - ì¸ì½”ë” ì§€ì—°: ~10ms âš¡ (6ë°° ë¹ ë¦„)
  - ë©”ëª¨ë¦¬: ~10MB ğŸ’¾ (25ë°° ì ˆì•½)
  - ì—°ì‚°ëŸ‰: O(1) ğŸš€ (ì¼ì •)
```

### í’ˆì§ˆ ìœ ì§€

- âœ… ë™ì¼í•œ Transformer ê¸°ë°˜
- âœ… ë™ì¼í•œ Multi-Head Attention
- âœ… ë©”ëª¨ë¦¬ ë±…í¬ë¡œ ì¥ê±°ë¦¬ ì˜ì¡´ì„± ìœ ì§€
- âœ… ê¸°ì¡´ ë””ì½”ë” ê·¸ëŒ€ë¡œ ì‚¬ìš©

---

## ğŸ”„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

### ë‹¨ê³„ë³„ ì ìš©

```
Phase 1: êµ¬í˜„ (1-2ì£¼)
  â”œâ”€ EmformerEncoderLayer êµ¬í˜„
  â”œâ”€ EmformerSpeechEncoder êµ¬í˜„
  â””â”€ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

Phase 2: í†µí•© (1ì£¼)
  â”œâ”€ EchoStreamModel ìƒì„±
  â”œâ”€ Agent ìˆ˜ì •
  â””â”€ í†µí•© í…ŒìŠ¤íŠ¸

Phase 3: í•™ìŠµ (2-4ì£¼)
  â”œâ”€ ê¸°ì¡´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ
  â”œâ”€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
  â””â”€ ì„±ëŠ¥ ê²€ì¦

Phase 4: í‰ê°€ (1ì£¼)
  â”œâ”€ BLEU, ASR-BLEU
  â”œâ”€ Latency (AL, AP, DAL)
  â””â”€ í’ˆì§ˆ ë¹„êµ
```

---

## ğŸ’¡ ê²°ë¡ 

### âœ… ê°€ëŠ¥ì„±

**ì™„ì „íˆ ê°€ëŠ¥í•©ë‹ˆë‹¤!** ì´ìœ :

1. **ëª¨ë“ˆì‹ ì„¤ê³„**: StreamSpeechëŠ” ì¸ì½”ë”/ë””ì½”ë”ê°€ ë…ë¦½ì 
2. **ë™ì¼í•œ ì¶œë ¥ í˜•ì‹**: ë‘˜ ë‹¤ `[T, B, 256]` ì¶œë ¥
3. **ê²€ì¦ëœ ê¸°ìˆ **: EmformerëŠ” ì´ë¯¸ fairseqì— êµ¬í˜„ë¨
4. **í˜¸í™˜ì„±**: ë‚˜ë¨¸ì§€ ì»´í¬ë„ŒíŠ¸ ìˆ˜ì • ë¶ˆí•„ìš”

### ğŸ“ˆ ì˜ˆìƒ ì´ì 

| ë©”íŠ¸ë¦­ | ê°œì„  |
|--------|------|
| **ì†ë„** | â¬†ï¸ 6-50ë°° (ë°œí™” ê¸¸ì´ì— ë”°ë¼) |
| **ë©”ëª¨ë¦¬** | â¬‡ï¸ 25ë°° ì ˆì•½ |
| **ì§€ì—°** | â¬‡ï¸ ì¼ì • (ë°œí™” ê¸¸ì´ ë¬´ê´€) |
| **í’ˆì§ˆ** | â¡ï¸ ìœ ì§€ ë˜ëŠ” ì†Œí­ í–¥ìƒ |
| **í™•ì¥ì„±** | â¬†ï¸ ê¸´ ë°œí™” ì²˜ë¦¬ ê°€ëŠ¥ |

### ğŸ¯ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

**EchoStreamì˜ í•µì‹¬**: Emformerì˜ Left Context Cache + Memory BankëŠ” StreamSpeechì˜ Chunk-based Conformerë¥¼ ì™„ë²½í•˜ê²Œ ëŒ€ì²´í•  ìˆ˜ ìˆìœ¼ë©°, í›¨ì”¬ íš¨ìœ¨ì ì…ë‹ˆë‹¤!

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. âœ… í”„ë¡œì íŠ¸ ì´ë¦„ í™•ì •: **EchoStream**
2. âœ… README ì—…ë°ì´íŠ¸ ì™„ë£Œ
3. â­ï¸ Emformer ëª¨ë“ˆ êµ¬í˜„ ì‹œì‘
4. â­ï¸ í†µí•© ë° í…ŒìŠ¤íŠ¸
5. â­ï¸ í•™ìŠµ ë° í‰ê°€

**EchoStream ê°œë°œì„ ì‹œì‘í•©ë‹ˆë‹¤!** ğŸŠ