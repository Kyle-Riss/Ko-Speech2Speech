# EchoStream ëª¨ë¸ ì•„í‚¤í…ì²˜ (ì •í™•í•œ êµ¬ì¡°)

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-02  
**ë²„ì „**: 1.0  
**ê¸°ë°˜**: StreamSpeech (ACL 2024) + Emformer Encoder

---

## ğŸ“Š ì „ì²´ íŒŒì´í”„ë¼ì¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        EchoStream S2ST Model                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input: Speech [B, T, 80]  (80-dim filter-bank, 16kHz, 10ms hop)
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Conv2D Subsampling (4x downsampling)       â”‚
â”‚    - Conv2D (k=3, s=2): 80 â†’ 40 features      â”‚
â”‚    - Conv2D (k=3, s=2): 40 â†’ 20 features      â”‚
â”‚    - Linear: 256*20 â†’ 256d                    â”‚
â”‚    Output: [T/4, B, 256]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Emformer Encoder (16 layers, 256d)         â”‚
â”‚                                                â”‚
â”‚    For each layer:                             â”‚
â”‚      - Left Context Cache (30 frames)         â”‚
â”‚      - Memory Bank (8 vectors from layer n-1) â”‚
â”‚      - Current Segment (4 frames)             â”‚
â”‚      - Multi-Head Attention (4 heads)         â”‚
â”‚      - Feed-Forward (1024d)                   â”‚
â”‚                                                â”‚
â”‚    Output: [T/4, B, 256]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                 â”‚                  â”‚
  â–¼                 â–¼                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ 3a. ASR CTC â”‚ â”‚ 3b. ST CTC      â”‚   â”‚
â”‚             â”‚ â”‚   + 2L Trans.   â”‚   â”‚
â”‚ Vocab: 6K   â”‚ â”‚   (unidirect.)  â”‚   â”‚
â”‚ (Source)    â”‚ â”‚   Vocab: 6K     â”‚   â”‚
â”‚             â”‚ â”‚   (Target)      â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
  â”‚                 â”‚                  â”‚
  â”‚ (punctuation)   â–¼                  â”‚
  â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
  â”‚            â”‚ 4. MT Decoder   â”‚     â”‚
  â”‚            â”‚   (4L Trans.)   â”‚     â”‚
  â”‚            â”‚                 â”‚â—„â”€â”€â”€â”€â”˜ (cross-attn)
  â”‚            â”‚   Vocab: 6K     â”‚
  â”‚            â”‚   (Target)      â”‚
  â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 5. Unit Decoder â”‚
    â”‚   (6L Trans.)   â”‚
    â”‚                 â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Encoder out (cross-attn)
    â”‚   + CTC Upsampleâ”‚
    â”‚   (ratio: 5)    â”‚
    â”‚                 â”‚
    â”‚   Units: 1000   â”‚
    â”‚   (HuBERT)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 6. CodeHiFiGAN  â”‚
    â”‚    Vocoder      â”‚
    â”‚                 â”‚
    â”‚  Units â†’ Wav    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
Output: Waveform [B, T_wav] @ 16kHz
```

---

## ğŸ”§ ì»´í¬ë„ŒíŠ¸ë³„ ìƒì„¸ êµ¬ì¡°

### 1. Conv2D Subsampler

**ëª©ì **: ì…ë ¥ featureë¥¼ 4ë°° ë‹¤ìš´ìƒ˜í”Œë§í•˜ì—¬ ê³„ì‚° íš¨ìœ¨ í–¥ìƒ

```python
Input:  [B, T, 80]  (80-dim filter-bank)
        â†“
Conv2D Layer 1:
  - in_channels: 1
  - out_channels: 256
  - kernel_size: 3
  - stride: 2
  - padding: 1
  Output: [B, 256, T/2, 40]
        â†“
ReLU
        â†“
Conv2D Layer 2:
  - in_channels: 256
  - out_channels: 256
  - kernel_size: 3
  - stride: 2
  - padding: 1
  Output: [B, 256, T/4, 20]
        â†“
Reshape: [B, T/4, 256*20] = [B, T/4, 5120]
        â†“
Linear: 5120 â†’ 256
        â†“
Transpose: [B, T/4, 256] â†’ [T/4, B, 256]
        â†“
Output: [T/4, B, 256]
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: ~1.3M

---

### 2. Emformer Encoder (í•µì‹¬!)

**êµ¬ì¡°**: 16ê°œì˜ ë™ì¼í•œ Emformer Layer

#### ë‹¨ì¼ Emformer Layer

```
Input (current segment):  [4, B, 256]  (4 frames @ 10ms = 40ms chunk)
Left Context Cache:       [30, B, 256] (previous segments' K, V)
Memory Bank:              [8, B, 256]  (from layer n-1)
Right Context:            [0, B, 256]  (streaming = no lookahead)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Prepare Q, K, V                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Query:                                     â”‚
â”‚    Q = segment  [4, B, 256]                 â”‚
â”‚                                             â”‚
â”‚  Key & Value:                               â”‚
â”‚    K = [memory_bank,  left_context,  seg]  â”‚
â”‚      = [8, B, 256] + [30, B, 256] + [4]    â”‚
â”‚      = [42, B, 256]  â† Efficient!           â”‚
â”‚                                             â”‚
â”‚    V = [memory_bank,  left_context,  seg]  â”‚
â”‚      = [42, B, 256]                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Multi-Head Attention               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  attn_output = MHA(Q, K, V)                 â”‚
â”‚    - num_heads: 4                           â”‚
â”‚    - head_dim: 256 / 4 = 64                 â”‚
â”‚    - dropout: 0.1                           â”‚
â”‚                                             â”‚
â”‚  Output: [4, B, 256]                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: Add & Norm                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  x = segment + dropout(attn_output)         â”‚
â”‚  x = LayerNorm(x)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Feed-Forward Network               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  FFN:                                       â”‚
â”‚    - Linear: 256 â†’ 1024                     â”‚
â”‚    - ReLU                                   â”‚
â”‚    - Dropout: 0.1                           â”‚
â”‚    - Linear: 1024 â†’ 256                     â”‚
â”‚                                             â”‚
â”‚  x = x + dropout(FFN(x))                    â”‚
â”‚  x = LayerNorm(x)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 5: Update Cache                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  left_context_cache_new = segment           â”‚
â”‚    (current segment â†’ next segment's left)  â”‚
â”‚                                             â”‚
â”‚  memory_bank_new = summarize(output)        â”‚
â”‚    (averaged output â†’ upper layer n+1)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
Output: [4, B, 256]
Cache: {left_context: [4, B, 256], memory: [1, B, 256]}
```

#### 16-Layer Emformer ì „ì²´

```
Layer 1:  segment[4] + left[30] + mem[8 from input]
            â†“
          cache: left[4], mem[1] â”€â”€â”
                                    â”‚
Layer 2:  segment[4] + left[30] + mem[1 from L1] â†â”˜
            â†“
          cache: left[4], mem[1] â”€â”€â”
                                    â”‚
Layer 3:  segment[4] + left[30] + mem[1 from L2] â†â”˜
            â†“
          ...
            â†“
Layer 16: segment[4] + left[30] + mem[1 from L15]
            â†“
          Final output: [4, B, 256]

Note: Left contextëŠ” ì´ì „ "ì‹œê°„" ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ
      Memory bankëŠ” ì´ì „ "ë ˆì´ì–´"ì—ì„œ ì „ë‹¬
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: ~14M

**í•µì‹¬ íŠ¹ì§•**:
- âœ… O(1) ë³µì¡ë„ (segment í¬ê¸° ê³ ì •)
- âœ… Left Context Cache (K, V ì¬ì‚¬ìš©)
- âœ… Memory Bank (í•˜ìœ„â†’ìƒìœ„ ë ˆì´ì–´ ì •ë³´ ì „ë‹¬)
- âœ… ê³ ì • ë©”ëª¨ë¦¬ (42 frames = 420ms context)

---

### 3a. ASR CTC Decoder

**ëª©ì **: Source ì–¸ì–´ í…ìŠ¤íŠ¸ ì˜ˆì¸¡ (punctuation predictionìš©)

```python
Input:  encoder_out [T/4, B, 256]
          â†“
Linear: 256 â†’ 6000  (source vocab)
          â†“
Log Softmax (dim=-1)
          â†“
Output: log_probs [T/4, B, 6000]

Decoding: Greedy CTC decoding
  â†’ Source text (for CT-Transformer punctuation)
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: 1.5M

---

### 3b. ST CTC Decoder (with Transformer)

**ëª©ì **: Target ì–¸ì–´ í…ìŠ¤íŠ¸ ì˜ˆì¸¡ (streaming translation)

```python
Input:  encoder_out [T/4, B, 256]
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Encoder Layer 1           â”‚
â”‚   - Self-Attention (causal mask)      â”‚
â”‚   - Feed-Forward                      â”‚
â”‚   Output: [T/4, B, 256]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Encoder Layer 2           â”‚
â”‚   - Self-Attention (causal mask)      â”‚
â”‚   - Feed-Forward                      â”‚
â”‚   Output: [T/4, B, 256]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
Linear: 256 â†’ 6000  (target vocab)
          â†“
Log Softmax
          â†“
Output: log_probs [T/4, B, 6000]

Decoding: Greedy CTC decoding
  â†’ Target text (preliminary translation)
```

**Causal Mask** (Unidirectional):
```
Attention mask:
  [1, 0, 0, 0]  â† frame 0 only sees frame 0
  [1, 1, 0, 0]  â† frame 1 sees frames 0-1
  [1, 1, 1, 0]  â† frame 2 sees frames 0-2
  [1, 1, 1, 1]  â† frame 3 sees frames 0-3

â†’ Streaming-friendly!
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: ~2.6M

---

### 4. MT Decoder (Transformer)

**ëª©ì **: CTC ì¶œë ¥ì„ autoregressiveí•˜ê²Œ refine

```python
Input:
  - prev_output_tokens: [B, T_tgt]  (shifted target)
  - encoder_out: [T/4, B, 256]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token Embedding                        â”‚
â”‚   - Embedding: 6000 â†’ 256             â”‚
â”‚   - Positional Encoding (sinusoidal)  â”‚
â”‚   Output: [T_tgt, B, 256]             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Decoder Layer 1            â”‚
â”‚   1. Self-Attention (causal mask)     â”‚
â”‚   2. Cross-Attention (to encoder)     â”‚
â”‚   3. Feed-Forward                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Decoder Layer 2            â”‚
â”‚   (same structure)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Decoder Layer 3            â”‚
â”‚   (same structure)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Decoder Layer 4            â”‚
â”‚   (same structure)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
Linear: 256 â†’ 6000
          â†“
Output: logits [B, T_tgt, 6000]

Decoding: Autoregressive (beam search or greedy)
  â†’ Refined target text
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: ~5.1M

---

### 5. Unit Decoder (CTC Transformer)

**ëª©ì **: Text hidden states â†’ Speech units

```python
Input:  text_hidden [T/4, B, 256]  (from encoder or MT decoder)
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CTC Upsampling (ratio: 5)              â”‚
â”‚   Repeat each frame 5 times            â”‚
â”‚   [T/4, B, 256] â†’ [5*T/4, B, 256]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input Projection                       â”‚
â”‚   Linear: 256 â†’ 256                   â”‚
â”‚   + Positional Encoding               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Decoder Layer 1            â”‚
â”‚   1. Self-Attention (causal)          â”‚
â”‚   2. Cross-Attention (to upsampled)   â”‚
â”‚   3. Feed-Forward                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
      (Layers 2-6)
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Decoder Layer 6            â”‚
â”‚   (same structure)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
Linear: 256 â†’ 1000  (HuBERT units)
          â†“
Log Softmax
          â†“
Output: log_probs [B, 5*T/4, 1000]

Decoding: Greedy
  â†’ Discrete speech units (0-999)
```

**CTC Upsampling**:
```
Input:  [a, b, c, d]  (4 frames)
          â†“
Repeat 5x: [a,a,a,a,a, b,b,b,b,b, c,c,c,c,c, d,d,d,d,d]
          â†“
Output: [20 frames]  (5ë°° ì¦ê°€)

â†’ Speech unitì˜ ì‹œê°„ í•´ìƒë„ë¥¼ ë†’ì„!
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: ~7.7M

---

### 6. CodeHiFiGAN Vocoder

**ëª©ì **: Discrete units â†’ Waveform

```python
Input:  units [B, T_unit]  (discrete unit IDs: 0-999)
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Unit Embedding                         â”‚
â”‚   units â†’ one-hot [B, T_unit, 1000]   â”‚
â”‚   (í˜„ì¬: dummy linear projection)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generator Network (GAN)                â”‚
â”‚   - Transposed convolutions           â”‚
â”‚   - Upsample to 16kHz                 â”‚
â”‚   - Multi-scale discriminators        â”‚
â”‚                                       â”‚
â”‚   (í˜„ì¬: dummy 32 samples/unit)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
Tanh (normalize to [-1, 1])
          â†“
Output: waveform [B, T_wav] @ 16kHz

T_wav = T_unit Ã— samples_per_unit
      = T_unit Ã— 32  (dummy)
      = (5*T/4) Ã— 32
```

**íŒŒë¼ë¯¸í„° ìˆ˜**: ~2.1M (dummy), ~14M (real CodeHiFiGAN)

**Note**: í˜„ì¬ëŠ” dummy vocoder, ì‹¤ì œë¡œëŠ” pre-trained CodeHiFiGAN í•„ìš”

---

## ğŸ“ ëª¨ë¸ í¬ê¸°

### íŒŒë¼ë¯¸í„° ìˆ˜ (16-layer full model)

| Component | Parameters | Percentage |
|-----------|-----------|------------|
| **Conv2D Subsampler** | 1,312,256 | 3.9% |
| **Emformer Encoder (16L)** | 15,592,448 | 46.0% |
| **ASR CTC Decoder** | 1,536,000 | 4.5% |
| **ST CTC Decoder (2L)** | 2,579,456 | 7.6% |
| **MT Decoder (4L)** | 5,136,384 | 15.2% |
| **Unit Decoder (6L)** | 7,699,721 | 22.7% |
| **Vocoder (dummy)** | 2,049 | 0.01% |
| **Total** | **33,858,314** | **100%** |

**ëª¨ë¸ í¬ê¸°**: ~33.9M parameters (~129 MB @ fp32)

**ë¹„êµ**:
- StreamSpeech Conformer: ~45M parameters
- EchoStream: 33.9M (ì•½ 25% ì‘ìŒ)

---

## âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ê¸°ë³¸ ì„¤ì •)

### Encoder (Emformer)

```yaml
encoder_embed_dim: 256
encoder_layers: 16
encoder_attention_heads: 4
encoder_ffn_embed_dim: 1024

# Emformer-specific
segment_length: 4          # 40ms @ 100fps
left_context_length: 30    # 300ms
right_context_length: 0    # Streaming (no lookahead)
memory_size: 8             # Memory bank size

# Input
input_feat_per_channel: 80  # Mel filter-bank
input_channels: 1
```

### Decoders

```yaml
# ST CTC
st_decoder_layers: 2
st_decoder_heads: 4
st_vocab_size: 6000

# MT Decoder
mt_decoder_layers: 4
mt_decoder_heads: 4
mt_decoder_embed_dim: 256
mt_decoder_ffn_dim: 1024
mt_vocab_size: 6000

# Unit Decoder
unit_decoder_layers: 6
unit_decoder_heads: 4
unit_decoder_embed_dim: 256
unit_decoder_ffn_dim: 1024
num_units: 1000
ctc_upsample_ratio: 5
```

### Regularization

```yaml
dropout: 0.1
attention_dropout: 0.1
activation_dropout: 0.1
label_smoothing: 0.1  # For training
```

---

## ğŸ”„ ë°ì´í„° íë¦„ (Forward Pass)

### Training Mode

```python
# Input
src_tokens = [B, T, 80]           # Speech features
src_lengths = [B]                  # Sequence lengths
prev_output_tokens = [B, T_tgt]   # Shifted target text

# 1. Encoder
encoder_out = encoder(src_tokens, src_lengths)
# â†’ [T/4, B, 256]

# 2. ASR CTC
asr_logits = asr_decoder(encoder_out)
# â†’ [T/4, B, 6000]
asr_loss = CTC_loss(asr_logits, src_text)

# 3. ST CTC
st_logits = st_decoder(encoder_out)
# â†’ [T/4, B, 6000]
st_loss = CTC_loss(st_logits, tgt_text)

# 4. MT Decoder
mt_logits = mt_decoder(prev_output_tokens, encoder_out)
# â†’ [B, T_tgt, 6000]
mt_loss = CrossEntropy(mt_logits, tgt_text)

# 5. Unit Decoder
unit_logits = unit_decoder(encoder_out)
# â†’ [B, 5*T/4, 1000]
unit_loss = CrossEntropy(unit_logits, tgt_units)

# Total Loss
loss = 0.3*asr_loss + 0.3*st_loss + 0.2*mt_loss + 0.2*unit_loss
```

### Inference Mode (Streaming)

```python
# Input: Audio chunks (40ms each)
for chunk in audio_stream:
    # 1. Extract features
    features = extract_features(chunk)  # [1, 4, 80]
    
    # 2. Encoder (with cache)
    encoder_out = encoder(features)  # [1, 1, 256]
    
    # 3. ST CTC (streaming)
    st_logits = st_decoder(encoder_out)
    st_text = ctc_decode(st_logits)  # Incremental
    
    # 4. Check punctuation (optional)
    punctuated, is_end = punctuator(st_text)
    
    # 5. Unit prediction
    unit_logits = unit_decoder(encoder_out)
    units = unit_logits.argmax(-1)
    
    # 6. Vocoder
    waveform = vocoder(units)
    
    # 7. Output
    if is_end:
        # Recompose full sentence
        final_waveform = recompose(buffered_units)
        yield final_waveform
    else:
        # Stream partial output
        yield waveform
```

---

## ğŸ¯ í•µì‹¬ ì„¤ê³„ ì›ì¹™

### 1. Efficient Streaming (Emformer)

**ë¬¸ì œ**: Conformerì˜ O(TÂ²) attention
**í•´ê²°**: Emformerì˜ O(1) attention
- Left Context Cache: ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ì˜ K, V ì¬ì‚¬ìš©
- Memory Bank: í•˜ìœ„ ë ˆì´ì–´ì—ì„œ ìš”ì•½ ì •ë³´ ì „ë‹¬
- Segment-wise processing: ê³ ì • í¬ê¸° (4 frames = 40ms)

### 2. Multi-task Learning

**ëª©ì **: ë‹¨ì¼ ëª¨ë¸ë¡œ ì—¬ëŸ¬ íƒœìŠ¤í¬ ë™ì‹œ í•™ìŠµ
- ASR: Source ì–¸ì–´ ì¸ì‹
- ST: ë²ˆì—­ (CTC)
- MT: ë²ˆì—­ refinement (autoregressive)
- Unit: Speech unit ì˜ˆì¸¡

**ì¥ì **:
- ê³µìœ  encoder â†’ íš¨ìœ¨ì 
- ìƒí˜¸ ë³´ì™„ì  í•™ìŠµ
- Intermediate supervision

### 3. Unidirectional Processing

**ìŠ¤íŠ¸ë¦¬ë° ìš”êµ¬ì‚¬í•­**:
- Emformer: right_context = 0
- ST CTC Decoder: causal mask
- MT Decoder: causal mask
- Unit Decoder: causal mask

**ê²°ê³¼**: ì™„ì „í•œ ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥!

### 4. CTC-based Policy

**ì¥ì **:
- Non-autoregressive (ë³‘ë ¬ ì²˜ë¦¬)
- Alignment ìë™ í•™ìŠµ
- Latency ì˜ˆì¸¡ ê°€ëŠ¥

**ì‚¬ìš©**:
- ASR output â†’ punctuation
- ST output â†’ preliminary translation
- Unit upsampling â†’ temporal resolution

---

## ğŸ†š StreamSpeechì™€ì˜ ì°¨ì´

| Component | StreamSpeech | EchoStream | ì°¨ì´ì  |
|-----------|-------------|-----------|-------|
| **Encoder** | Chunk-based Conformer | Emformer | O(TÂ²) â†’ O(1) |
| **Complexity** | Quadratic | Constant | ë°œí™” ê¸¸ì´ì— ë¬´ê´€ |
| **Memory** | ì¦ê°€ (ë°œí™”â†‘) | ê³ ì • (42 frames) | 13,000ë°° ì ˆì•½ |
| **Cache** | None | Left Context + Memory | K, V ì¬ì‚¬ìš© |
| **Decoders** | ë™ì¼ | ë™ì¼ | 100% ë™ì¼ |
| **Vocoder** | CodeHiFiGAN | CodeHiFiGAN | ë™ì¼ |
| **Parameters** | ~45M | ~34M | 25% ê°ì†Œ |

**ê²°ë¡ **: ì¸ì½”ë”ë§Œ êµì²´, ë””ì½”ë”ëŠ” ì™„ì „ ë™ì¼!

---

## ğŸ“ êµ¬í˜„ íŒŒì¼

```
models/
â”œâ”€â”€ emformer_layer.py          # EmformerEncoder (16L)
â”œâ”€â”€ echostream_encoder.py      # Conv2D + Emformer
â”œâ”€â”€ echostream_model.py        # Complete model
â””â”€â”€ decoders/
    â”œâ”€â”€ ctc_decoder.py         # ASR CTC + ST CTC
    â”œâ”€â”€ transformer_decoder.py # MT Decoder
    â”œâ”€â”€ unit_decoder.py        # Unit Decoder
    â””â”€â”€ vocoder.py             # CodeHiFiGAN (dummy)

configs/
â””â”€â”€ echostream_config.yaml     # Hyperparameters

agent/
â””â”€â”€ echostream_agent.py        # SimulEval agent

scripts/
â”œâ”€â”€ train.py                   # Training
â””â”€â”€ evaluate.py                # Evaluation
```

---

## ğŸš€ ì‚¬ìš© ì˜ˆì‹œ

### ëª¨ë¸ ìƒì„±

```python
from models.echostream_model import build_echostream_model, EchoStreamConfig

# Create config
config = EchoStreamConfig()
config.encoder_layers = 16  # Full model

# Build model
model = build_echostream_model(config)
model.eval()

print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
```

### Forward Pass

```python
import torch

# Input
B, T, F = 2, 100, 80
src_tokens = torch.randn(B, T, F)
src_lengths = torch.tensor([100, 80])

# Forward
with torch.no_grad():
    output = model(src_tokens, src_lengths)

# Outputs
print(f"Encoder: {output['encoder_out']['encoder_out'][0].shape}")  # [25, 2, 256]
print(f"ASR: {output['asr_logits'].shape}")      # [2, 25, 6000]
print(f"ST: {output['st_logits'].shape}")        # [2, 25, 6000]
print(f"Units: {output['unit_logits'].shape}")   # [2, 125, 1000]
print(f"Waveform: {output['waveform'].shape}")   # [2, 4000]
```

### Streaming

```python
# Reset cache
model.reset_cache()

# Process chunks
for chunk in audio_chunks:  # Each chunk: [1, 4, 80] (40ms)
    output = model(chunk, lengths=torch.tensor([4]))
    waveform = output['waveform']
    # Stream output...
```

---

**EchoStream**: Efficient + Echo(ë°˜ë³µ ì—†ëŠ” ìºì‹œ) + Stream(ì‹¤ì‹œê°„)! ğŸŒŠ

