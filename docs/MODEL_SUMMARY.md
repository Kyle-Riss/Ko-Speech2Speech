# EchoStream λ¨λΈ κµ¬μ΅° (ν•λμ— λ³΄κΈ°)

## π― ν•µμ‹¬ μ”μ•½

**EchoStream** = **Emformer Encoder** + **StreamSpeech Decoders**

```
Speech β†’ Emformer (NEW!) β†’ ASR/ST/MT/Unit Decoders (SAME) β†’ Waveform
         β†‘ O(1) λ³µμ΅λ„
         β†‘ Left Context Cache
         β†‘ Memory Bank
```

---

## π“ μ „μ²΄ κµ¬μ΅° (κ°„λ‹¨ λ²„μ „)

```
Input: Speech [B, T, 80]
    β†“
[1] Conv2D Subsampling (4x)
    β†’ [T/4, B, 256]
    β†“
[2] Emformer Encoder (16 layers)
    β†’ [T/4, B, 256]
    β†“
    β”β”€β”€β†’ [3a] ASR CTC β†’ Source text (for punctuation)
    β”‚
    β””β”€β”€β†’ [3b] ST CTC (2L Trans) β†’ Target text
           β†“
         [4] MT Decoder (4L Trans) β†’ Refined text
           β†“
         [5] Unit Decoder (6L Trans) β†’ Speech units
           β†“
         [6] CodeHiFiGAN β†’ Waveform
           β†“
Output: Speech
```

---

## π”§ κ° μ»΄ν¬λ„νΈ

### 1οΈβƒ£ Conv2D Subsampling
- **μ…λ ¥**: [B, T, 80] (80-dim filter-bank)
- **μ¶λ ¥**: [T/4, B, 256]
- **κΈ°λ¥**: 4λ°° λ‹¤μ΄μƒν”λ§ (ν¨μ¨μ„±β†‘)
- **νλΌλ―Έν„°**: 1.3M

### 2οΈβƒ£ Emformer Encoder (ν•µμ‹¬!)
- **κµ¬μ΅°**: 16κ° λ μ΄μ–΄
- **μ°¨μ›**: 256d, 4 heads, 1024d FFN
- **νΉμ§•**:
  ```
  Segment: 4 frames (40ms)
  Left Context: 30 frames (300ms) β† μΊμ‹ μ¬μ‚¬μ©!
  Memory Bank: 8 vectors β† ν•μ„ λ μ΄μ–΄μ—μ„
  ```
- **λ³µμ΅λ„**: **O(1)** (vs Conformer O(TΒ²))
- **νλΌλ―Έν„°**: 15.6M

### 3οΈβƒ£ Decoders

#### 3a. ASR CTC Decoder
- **κΈ°λ¥**: Source μ–Έμ–΄ μΈμ‹
- **μ¶λ ¥**: [T/4, B, 6000] (source vocab)
- **μ©λ„**: Punctuation prediction
- **νλΌλ―Έν„°**: 1.5M

#### 3b. ST CTC Decoder
- **κµ¬μ΅°**: 2-layer Transformer + CTC
- **κΈ°λ¥**: Target μ–Έμ–΄ λ²μ—­ (preliminary)
- **μ¶λ ¥**: [T/4, B, 6000] (target vocab)
- **νΉμ§•**: Unidirectional (streaming!)
- **νλΌλ―Έν„°**: 2.6M

#### 4. MT Decoder
- **κµ¬μ΅°**: 4-layer Transformer
- **κΈ°λ¥**: Text refinement (autoregressive)
- **μ¶λ ¥**: [B, T_tgt, 6000]
- **νλΌλ―Έν„°**: 5.1M

#### 5. Unit Decoder
- **κµ¬μ΅°**: 6-layer Transformer + CTC Upsample
- **κΈ°λ¥**: Text β†’ Speech units
- **μ¶λ ¥**: [B, 5Γ—T/4, 1000] (HuBERT units)
- **CTC Upsample**: 5λ°° μ¦κ°€ (μ‹κ°„ ν•΄μƒλ„β†‘)
- **νλΌλ―Έν„°**: 7.7M

#### 6. CodeHiFiGAN Vocoder
- **κΈ°λ¥**: Units β†’ Waveform
- **μ¶λ ¥**: [B, T_wav] @ 16kHz
- **νλΌλ―Έν„°**: 2.1M (dummy), ~14M (real)

---

## π“ λ¨λΈ ν¬κΈ°

```
μ΄ νλΌλ―Έν„°: 33.9M
λ¨λΈ ν¬κΈ°:   ~129 MB (fp32)

κµ¬μ„±:
  Encoder:     15.6M  (46%)  β† Emformer
  ST CTC:       2.6M  (8%)
  MT:           5.1M  (15%)
  Unit:         7.7M  (23%)
  κΈ°νƒ€:         2.9M  (8%)
```

**λΉ„κµ**:
- StreamSpeech: 45M
- EchoStream: 34M (**25% κ°μ†**)

---

## β΅ ν•µμ‹¬ μ°¨μ΄μ  (vs StreamSpeech)

| ν•­λ© | StreamSpeech | EchoStream |
|-----|-------------|-----------|
| **Encoder** | Chunk Conformer | **Emformer** β­ |
| **Complexity** | O(TΒ²) | **O(1)** β­ |
| **Memory (10s)** | ~256 MB | **~10 MB** β­ |
| **Latency (10s)** | ~1,262 ms | **803 ms** β­ |
| **Scaling** | Quadratic | **Linear** β­ |
| **Decoders** | Same | Same β… |
| **Quality** | SOTA | Same β… |

**κ²°λ΅ **: μΈμ½”λ”λ§ κµμ²΄ β†’ ν¨μ¨μ„± λ€ν­ ν–¥μƒ!

---

## π”„ λ™μ‘ λ°©μ‹

### Training

```python
loss = 0.3 Γ— ASR_CTC_loss
     + 0.3 Γ— ST_CTC_loss
     + 0.2 Γ— MT_loss
     + 0.2 Γ— Unit_loss
```

Multi-task learningμΌλ΅ λ™μ‹ ν•™μµ!

### Inference (Streaming)

```python
while audio_stream:
    chunk = read_40ms()              # [1, 4, 80]
    
    # Encoder
    enc_out = encoder(chunk)         # [1, 1, 256]
                                     # β†‘ Cache μ¬μ‚¬μ©!
    
    # ST CTC
    text = st_decoder(enc_out)       # Incremental
    
    # Punctuation check
    if is_sentence_end(text):
        # Recompose
        final_units = unit_decoder(buffered_text)
        final_wav = vocoder(final_units)
        output(final_wav)
    else:
        # Stream
        units = unit_decoder(enc_out)
        wav = vocoder(units)
        output(wav)
```

---

## π’΅ μ™ λΉ λ¥Έκ°€?

### Conformer (StreamSpeech)

```
Chunk 1:  attention([c0])           β†’ 1 κ³„μ‚°
Chunk 2:  attention([c0, c1])       β†’ 2 κ³„μ‚°
Chunk 3:  attention([c0, c1, c2])   β†’ 3 κ³„μ‚°
...
Chunk 100: attention([c0, ..., c99]) β†’ 100 κ³„μ‚°

Total: 1+2+3+...+100 = 5,050 β
```

### Emformer (EchoStream)

```
Seg 1:  Q, K_new, V_new = compute(s0)   β†’ 1 κ³„μ‚°
        K = [cache, K_new]  (cache μ¬μ‚¬μ©!)
        V = [cache, V_new]

Seg 2:  Q, K_new, V_new = compute(s1)   β†’ 1 κ³„μ‚°
        K = [cache, K_new]
        V = [cache, V_new]
...
Seg 100: ...                             β†’ 1 κ³„μ‚°

Total: 1+1+1+...+1 = 100 β…
```

**μ°¨μ΄**: **50λ°° μ—°μ‚°λ‰ κ°μ†!**

---

## π― μ‚¬μ© μ‹λ‚λ¦¬μ¤

### β… EchoStream μ¶”μ²

- μ¤‘κ°„/κΈ΄ λ°ν™” (> 5μ΄)
- λ©”λ¨λ¦¬ μ μ•½ ν™κ²½
- ν”„λ΅λ•μ… λ°°ν¬
- μ—°μ† λ€ν™” μ‹μ¤ν…

### β οΈ StreamSpeech μ¶”μ²

- μ§§μ€ λ°ν™”λ§ (< 3μ΄)
- μ—°κµ¬μ© baseline
- Pre-trained λ¨λΈ ν•„μ”

---

## π“ νμΌ κµ¬μ΅°

```
models/
β”β”€β”€ emformer_layer.py       # Emformer ν•µμ‹¬
β”β”€β”€ echostream_encoder.py   # Conv2D + Emformer
β”β”€β”€ echostream_model.py     # μ „μ²΄ λ¨λΈ
β””β”€β”€ decoders/
    β”β”€β”€ ctc_decoder.py
    β”β”€β”€ transformer_decoder.py
    β”β”€β”€ unit_decoder.py
    β””β”€β”€ vocoder.py

configs/
β””β”€β”€ echostream_config.yaml  # ν•μ΄νΌνλΌλ―Έν„°

agent/
β””β”€β”€ echostream_agent.py     # SimulEval

scripts/
β”β”€β”€ train.py
β””β”€β”€ evaluate.py
```

---

## π€ Quick Start

```python
from models.echostream_model import build_echostream_model, EchoStreamConfig

# 1. Create model
config = EchoStreamConfig()
model = build_echostream_model(config)

# 2. Input
import torch
speech = torch.randn(1, 100, 80)  # 1s audio
lengths = torch.tensor([100])

# 3. Forward
output = model(speech, lengths)

# 4. Get waveform
waveform = output['waveform']  # [1, ~4000] @ 16kHz
```

---

## π“ μƒμ„Έ λ¬Έμ„

λ” μμ„Έν• λ‚΄μ©μ€:
- **ECHOSTREAM_ARCHITECTURE.md** - μ „μ²΄ μ•„ν‚¤ν…μ²
- **COMPARISON_STREAMSPEECH_VS_ECHOSTREAM.md** - λΉ„κµ λ¶„μ„
- **BENCHMARK_RESULTS.md** - μ„±λ¥ μΈ΅μ •

---

**EchoStream**: ν¨μ¨μ μ΄κ³  λΉ λ¥Έ μ‹¤μ‹κ°„ μμ„± λ²μ—­! π

