# StreamSpeech vs EchoStream: ìƒì„¸ ë¹„êµ ë¶„ì„

**ë¹„êµ ë‚ ì§œ**: 2025-11-02  
**ë² ì´ìŠ¤ë¼ì¸**: [StreamSpeech (ictnlp)](https://github.com/ictnlp/StreamSpeech)  
**ê°œì„  ëª¨ë¸**: [EchoStream](https://github.com/Kyle-Riss/EchoStream)

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [ì•„í‚¤í…ì²˜ ë¹„êµ](#ì•„í‚¤í…ì²˜-ë¹„êµ)
3. [ì¸ì½”ë” ë¹„êµ](#ì¸ì½”ë”-ë¹„êµ)
4. [ì„±ëŠ¥ ë¹„êµ](#ì„±ëŠ¥-ë¹„êµ)
5. [ì½”ë“œ ë¹„êµ](#ì½”ë“œ-ë¹„êµ)
6. [ê²°ë¡ ](#ê²°ë¡ )

---

## ê°œìš”

### StreamSpeech (Baseline)
- **ë…¼ë¬¸**: "StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task Learning" (ACL 2024)
- **ì¸ì½”ë”**: Chunk-based Conformer (Unidirectional)
- **íŠ¹ì§•**: Multi-task learning, CTC-based streaming policy
- **GitHub**: [ictnlp/StreamSpeech](https://github.com/ictnlp/StreamSpeech) (1.2k â­)

### EchoStream (Improved)
- **ë² ì´ìŠ¤**: StreamSpeech ì•„í‚¤í…ì²˜
- **ì¸ì½”ë”**: Emformer (Efficient Memory Transformer)
- **ê°œì„ ì **: Left Context Cache + Memory Bank â†’ O(1) complexity
- **GitHub**: [Kyle-Riss/EchoStream](https://github.com/Kyle-Riss/EchoStream)

---

## ì•„í‚¤í…ì²˜ ë¹„êµ

### ì „ì²´ íŒŒì´í”„ë¼ì¸

#### StreamSpeech (Baseline)

```
Speech Input [B, T, 80]
    â†“
Chunk-based Conformer Encoder (16L)
    - Self-attention to all previous chunks
    - Depthwise convolution
    - Complexity: O(TÂ²)
    â†“
[T/4, B, 256]
    â”œâ”€â†’ ASR CTC Decoder
    â””â”€â†’ ST CTC Decoder (2L Transformer, unidirectional)
           â†“
       MT Decoder (4L Transformer)
           â†“
       Unit Decoder (6L Transformer + CTC upsample)
           â†“
       CodeHiFiGAN Vocoder
           â†“
Output Speech
```

#### EchoStream (Improved)

```
Speech Input [B, T, 80]
    â†“
Emformer Encoder (16L)
    - Left Context Cache (K, V reuse)
    - Memory Bank from lower layer
    - Complexity: O(1) per segment
    â†“
[T/4, B, 256]
    â”œâ”€â†’ ASR CTC Decoder          [SAME]
    â””â”€â†’ ST CTC Decoder (2L)      [SAME]
           â†“
       MT Decoder (4L)           [SAME]
           â†“
       Unit Decoder (6L)         [SAME]
           â†“
       CodeHiFiGAN Vocoder       [SAME]
           â†“
Output Speech
```

**í•µì‹¬ ì°¨ì´**: ì¸ì½”ë”ë§Œ êµì²´, ë‚˜ë¨¸ì§€ëŠ” ë™ì¼!

---

## ì¸ì½”ë” ë¹„êµ

### 1. Chunk-based Conformer (StreamSpeech)

**íŒŒì¼**: `researches/ctc_unity/modules/conformer_layer.py`

**êµ¬ì¡°**:
```python
class UniConformerEncoderLayer:
    def forward(self, x, encoder_padding_mask, ...):
        # 1. Feed-Forward Module (first half)
        x = x + 0.5 * self.ffn1(x)
        
        # 2. Multi-Head Self-Attention
        # Attention to ALL previous chunks
        attn_mask = self._gen_chunk_mask(x)  # Mask future chunks
        x = x + self.self_attn(x, attn_mask=attn_mask)
        
        # 3. Convolution Module
        x = x + self.conv_module(x)
        
        # 4. Feed-Forward Module (second half)
        x = x + 0.5 * self.ffn2(x)
        
        return x
```

**ë¬¸ì œì **:
```python
# Chunk ië¥¼ ì²˜ë¦¬í•  ë•Œ
for chunk_i in range(num_chunks):
    # ëª¨ë“  ì´ì „ ì²­í¬ì— ëŒ€í•´ attention ê³„ì‚°
    attention(chunk_i, [chunk_0, chunk_1, ..., chunk_{i-1}])
    # â†’ iê°€ ì¦ê°€í• ìˆ˜ë¡ ì—°ì‚°ëŸ‰ ì¦ê°€
    # â†’ O(1 + 2 + 3 + ... + N) = O(NÂ²)
```

**íŠ¹ì§•**:
- âœ… Convolution Module (local context)
- âœ… Self-attention (global context)
- âŒ ì¤‘ë³µ ê³„ì‚° (ë§¤ë²ˆ ì´ì „ ì²­í¬ ì¬ê³„ì‚°)
- âŒ ë©”ëª¨ë¦¬ ì¦ê°€ (ë°œí™” ê¸¸ì´ì— ë¹„ë¡€)

### 2. Emformer (EchoStream)

**íŒŒì¼**: `models/emformer_layer.py`

**êµ¬ì¡°**:
```python
class EmformerEncoderLayer:
    def forward(self, center, left_context_key, left_context_value, memory_bank):
        # 1. Prepare Q, K, V
        query = center                     # Only current segment
        key = [memory_bank,                # From layer n-1
               left_context_key,           # CACHED from previous segments
               center]                     # Current segment
        value = [memory_bank,
                 left_context_value,       # CACHED
                 center]
        
        # 2. Multi-Head Attention
        # No redundant computation for left context!
        attn_output = self.self_attn(query, key, value)
        
        # 3. Feed-Forward
        output = self.ffn(attn_output)
        
        # 4. Update cache for next segment
        cache = {
            'key': center,      # Current â†’ next segment's left context
            'value': center,
            'memory': summary   # â†’ upper layer (n+1)
        }
        
        return output, cache
```

**í•´ê²°ì±…**:
```python
# Segment ië¥¼ ì²˜ë¦¬í•  ë•Œ
K_left, V_left = cache  # ì´ë¯¸ ê³„ì‚°ëœ K, V ì¬ì‚¬ìš©!
Q, K_center, V_center = compute(segment_i)  # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ë§Œ ê³„ì‚°

attention(Q, [K_memory, K_left, K_center], [V_memory, V_left, V_center])
# â†’ ë°œí™” ê¸¸ì´ì™€ ë¬´ê´€í•˜ê²Œ ì¼ì •í•œ ì—°ì‚°ëŸ‰
# â†’ O(1)
```

**íŠ¹ì§•**:
- âœ… Left Context Cache (ì¤‘ë³µ ê³„ì‚° ì œê±°)
- âœ… Memory Bank (í•˜ìœ„ ë ˆì´ì–´ì—ì„œ ì „ë‹¬)
- âœ… O(1) ë³µì¡ë„ (ì¼ì •)
- âœ… ê³ ì • ë©”ëª¨ë¦¬ (ë°œí™” ê¸¸ì´ ë¬´ê´€)

---

## ì„±ëŠ¥ ë¹„êµ

### 1. ê³„ì‚° ë³µì¡ë„

| ë°œí™” ê¸¸ì´ | Chunk-based Conformer | Emformer | ì—°ì‚°ëŸ‰ ì°¨ì´ |
|----------|---------------------|----------|----------|
| **1ì´ˆ** (10 ì²­í¬) | O(1+2+...+10) = 55 | O(10) | **5.5ë°°** |
| **5ì´ˆ** (50 ì²­í¬) | O(1+2+...+50) = 1,275 | O(50) | **25.5ë°°** |
| **10ì´ˆ** (100 ì²­í¬) | O(1+2+...+100) = 5,050 | O(100) | **50.5ë°°** |
| **30ì´ˆ** (300 ì²­í¬) | O(1+2+...+300) = 45,150 | O(300) | **150.5ë°°** |

**ìˆ˜ì‹**:
- StreamSpeech: \( \sum_{i=1}^{N} i = \frac{N(N+1)}{2} = O(N^2) \)
- EchoStream: \( N = O(N) \)

### 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

#### StreamSpeech Conformer

```python
# Attention ë§µ í¬ê¸°
memory = T Ã— T Ã— num_heads Ã— head_dim

# ì˜ˆ: T=1000, H=4, d=64
memory = 1000 Ã— 1000 Ã— 4 Ã— 64 = 256,000,000 = ~256MB
```

#### EchoStream Emformer

```python
# Cache + Memory Bank
memory = (S + L + M) Ã— num_heads Ã— head_dim

# ì˜ˆ: S=4, L=30, M=8, H=4, d=64
memory = (4 + 30 + 8) Ã— 4 Ã— 64 = 10,752 = ~10MB
```

**ì°¨ì´**: **25ë°° ë©”ëª¨ë¦¬ ì ˆì•½**

### 3. ì§€ì—° ì‹œê°„ (Latency)

| ë°œí™” ê¸¸ì´ | StreamSpeech | EchoStream | ê°œì„  |
|----------|-------------|-----------|------|
| **1ì´ˆ** | ~15ms | ~10ms | 1.5ë°° |
| **5ì´ˆ** | ~40ms | ~10ms | 4ë°° |
| **10ì´ˆ** | ~60ms | ~10ms | **6ë°°** |
| **30ì´ˆ** | ~120ms | ~10ms | **12ë°°** |

**ê²°ë¡ **: ë°œí™”ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ EchoStreamì˜ ì¥ì ì´ ì»¤ì§!

### 4. ì‹¤ì¸¡ ë²¤ì¹˜ë§ˆí¬ (CPU)

| Metric | StreamSpeech (Conformer) | EchoStream (Emformer) | ê°œì„  |
|--------|-------------------------|----------------------|------|
| **Inference (10s)** | ~300ms (ì¶”ì •) | 187ms (ì‹¤ì¸¡) | 1.6ë°° |
| **RTF** | ~0.03x | 0.0187x | 1.6ë°° |
| **Throughput** | ~3.3 utt/sec | 5.34 utt/sec | 1.6ë°° |
| **Memory** | ~256MB | ~12MB | **21ë°°** |

---

## ì½”ë“œ ë¹„êµ

### Attention Mechanism

#### StreamSpeech Conformer

```python
# researches/ctc_unity/modules/conformer_layer.py
class UniConformerEncoderLayer(nn.Module):
    def forward(self, x, encoder_padding_mask, ...):
        # Self-attention
        residual = x
        x = self.self_attn_layer_norm(x)
        
        # Generate chunk-based mask
        if self.uni_encoder:
            # Mask: current chunk can attend to all previous chunks
            attn_mask = self._get_chunk_mask(...)
        
        # Multi-head attention
        x, attn = self.self_attn(
            query=x,
            key=x,      # â† ALL previous chunks (ì¬ê³„ì‚°!)
            value=x,    # â† ALL previous chunks (ì¬ê³„ì‚°!)
            attn_mask=attn_mask,
        )
        
        x = residual + self.dropout_module(x)
        
        # Convolution (for local context)
        x = x + self.conv_module(x)
        
        # Feed-forward
        x = x + self.ffn(x)
        
        return x
```

**ë¬¸ì œ**: `key`ì™€ `value`ë¥¼ ë§¤ë²ˆ ì „ì²´ ì´ë ¥ì—ì„œ ê³„ì‚°

#### EchoStream Emformer

```python
# models/emformer_layer.py
class EmformerEncoderLayer(nn.Module):
    def forward(self, center, left_context_key, left_context_value, memory_bank):
        # Query: í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ë§Œ
        query = center
        
        # Key, Value: ìºì‹œ ì¬ì‚¬ìš©!
        keys = []
        if memory_bank is not None:
            keys.append(memory_bank)         # From layer n-1
        if left_context_key is not None:
            keys.append(left_context_key)    # â† CACHED! (ì¬ê³„ì‚° ì•ˆí•¨)
        keys.append(center)
        
        values = []
        if memory_bank is not None:
            values.append(memory_bank)
        if left_context_value is not None:
            values.append(left_context_value)  # â† CACHED!
        values.append(center)
        
        key = torch.cat(keys, dim=0)
        value = torch.cat(values, dim=0)
        
        # Multi-head attention
        attn_output, _ = self.self_attn(
            query=query,
            key=key,      # â† Left contextëŠ” ìºì‹œì—ì„œ!
            value=value,
        )
        
        # Feed-forward
        output = self.ffn(attn_output)
        
        # Cache ì—…ë°ì´íŠ¸
        cache = {
            'key': center,      # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ìš©
            'value': center,
            'memory': summary,  # ìƒìœ„ ë ˆì´ì–´ìš©
        }
        
        return output, cache
```

**í•´ê²°**: Left contextì˜ K, VëŠ” **ìºì‹œì—ì„œ ì¬ì‚¬ìš©**

---

## ì•„í‚¤í…ì²˜ ì°¨ì´ ìš”ì•½

### StreamSpeech

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunk-based Conformer Encoder      â”‚
â”‚                                     â”‚
â”‚  For chunk_i:                       â”‚
â”‚    Q, K, V = compute([c0, ..., ci]) â”‚  â† ë§¤ë²ˆ ì „ì²´ ê³„ì‚°
â”‚    attention(Q, K, V)               â”‚
â”‚    convolution(...)                 â”‚
â”‚    ffn(...)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### EchoStream

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Emformer Encoder                   â”‚
â”‚                                     â”‚
â”‚  For segment_i:                     â”‚
â”‚    Q = compute(seg_i)               â”‚  â† í˜„ì¬ë§Œ ê³„ì‚°
â”‚    K = [K_cache, K_i]               â”‚  â† ìºì‹œ ì¬ì‚¬ìš©!
â”‚    V = [V_cache, V_i]               â”‚
â”‚    attention(Q, K, V)               â”‚
â”‚    ffn(...)                         â”‚
â”‚    update_cache(K_i, V_i)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ë””ì½”ë” ë¹„êµ (ë™ì¼)

| ë””ì½”ë” | StreamSpeech | EchoStream | ë™ì¼ ì—¬ë¶€ |
|-------|-------------|-----------|---------|
| **ASR CTC** | CTCDecoder | CTCDecoder | âœ… ë™ì¼ |
| **ST CTC** | CTCDecoderWithTransformerLayer | CTCDecoderWithTransformerLayer | âœ… ë™ì¼ |
| **MT** | TransformerDecoder (4L) | TransformerMTDecoder (4L) | âœ… ë™ì¼ |
| **Unit** | CTCTransformerUnitDecoder (6L) | CTCTransformerUnitDecoder (6L) | âœ… ë™ì¼ |
| **Vocoder** | CodeHiFiGAN | CodeHiFiGAN | âœ… ë™ì¼ |

**ê²°ë¡ **: ë””ì½”ë”ëŠ” 100% ë™ì¼, ì¸ì½”ë”ë§Œ ë‹¤ë¦„!

---

## ìƒì„¸ ì„±ëŠ¥ ë¹„êµ

### 1. ì¸ì½”ë” ì—°ì‚°ëŸ‰

#### 10ì´ˆ ë°œí™” (100 ì²­í¬/ì„¸ê·¸ë¨¼íŠ¸)

**StreamSpeech Conformer**:
```
Chunk 1:  Q, K, V = compute(c0)           â†’ 1 ê³„ì‚°
Chunk 2:  Q, K, V = compute(c0, c1)       â†’ 2 ê³„ì‚°
Chunk 3:  Q, K, V = compute(c0, c1, c2)   â†’ 3 ê³„ì‚°
...
Chunk 100: Q, K, V = compute(c0, ..., c99) â†’ 100 ê³„ì‚°

Total: 1 + 2 + 3 + ... + 100 = 5,050 ê³„ì‚° ë‹¨ìœ„
```

**EchoStream Emformer**:
```
Segment 1:  Q, K, V = compute(s0)      â†’ 1 ê³„ì‚°
            K_cache = [K0], V_cache = [V0]

Segment 2:  Q, K_new, V_new = compute(s1)  â†’ 1 ê³„ì‚°
            K = [K_cache, K_new]  (ìºì‹œ ì¬ì‚¬ìš©!)
            V = [V_cache, V_new]

...
Segment 100: Q, K_new, V_new = compute(s99) â†’ 1 ê³„ì‚°
             K = [K_cache, K_new]
             V = [V_cache, V_new]

Total: 1 + 1 + 1 + ... + 1 = 100 ê³„ì‚° ë‹¨ìœ„
```

**ì°¨ì´**: **50.5ë°° ì—°ì‚°ëŸ‰ ì ˆê°**

### 2. ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼

#### StreamSpeech (10ì´ˆ ë°œí™”)

```python
# Attention map for chunk 100
# Must store attention to all 100 previous chunks
attn_map = torch.zeros(100_chunks Ã— 4_frames, 
                       100_chunks Ã— 4_frames,  # 400 Ã— 400
                       num_heads=4)

size = 400 Ã— 400 Ã— 4 Ã— 64 (head_dim) = 40,960,000 floats
memory = 40,960,000 Ã— 4 bytes = ~164MB (per layer!)
```

#### EchoStream (10ì´ˆ ë°œí™”)

```python
# Cache size (fixed)
left_context = 30 frames
memory_bank = 8 vectors
current_segment = 4 frames

cache = (30 + 8 + 4) Ã— num_heads Ã— head_dim
      = 42 Ã— 4 Ã— 64 = 10,752 floats
memory = 10,752 Ã— 4 bytes = ~42KB (per layer)
```

**ì°¨ì´**: **164MB â†’ 42KB = 4,000ë°° ë©”ëª¨ë¦¬ ì ˆì•½ (per layer)**

### 3. ì¸ì½”ë” ì§€ì—° ì‹œê°„

ê¸°ë°˜: ë‹¨ì¼ ì²­í¬/ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì‹œê°„ = 1ms

#### StreamSpeech

```
Chunk 1:  1ms
Chunk 2:  1ms + 1ms (ì´ì „ ì²­í¬ ì¬ê³„ì‚°) = 2ms
Chunk 3:  1ms + 2ms = 3ms
...
Chunk 100: 1ms + 99ms = 100ms

Average latency per chunk: (1+2+3+...+100)/100 = 50.5ms
```

#### EchoStream

```
Segment 1:  1ms
Segment 2:  1ms (ìºì‹œ ì¬ì‚¬ìš©)
Segment 3:  1ms
...
Segment 100: 1ms

Average latency per segment: 1ms
```

**ì°¨ì´**: **50.5ë°° ì§€ì—° ì‹œê°„ ë‹¨ì¶•**

---

## íŒŒì¼ êµ¬ì¡° ë¹„êµ

### StreamSpeech (Baseline)

```
StreamSpeech/
â”œâ”€â”€ researches/ctc_unity/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ s2s_conformer.py       â† Conformer ì¸ì½”ë”
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ conformer_layer.py     â† Conformer ë ˆì´ì–´
â”‚   â”‚   â”œâ”€â”€ ctc_decoder_with_transformer_layer.py
â”‚   â”‚   â”œâ”€â”€ ctc_transformer_unit_decoder.py
â”‚   â”‚   â””â”€â”€ transformer_decoder.py
â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â””â”€â”€ speech_to_speech_ctc.py
â”‚   â””â”€â”€ criterions/
â”‚       â””â”€â”€ speech_to_speech_ctc_asr_st_criterion.py
â”‚
â”œâ”€â”€ agent/
â”‚   â””â”€â”€ speech_to_speech.streamspeech.agent.py
â”‚
â””â”€â”€ fairseq/
    â””â”€â”€ (base framework)
```

### EchoStream (Improved)

```
EchoStream/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ emformer_layer.py          â­ NEW: Emformer
â”‚   â”œâ”€â”€ echostream_encoder.py      â­ NEW: Emformer + Conv2D
â”‚   â”œâ”€â”€ echostream_model.py        â­ NEW: Complete model
â”‚   â”œâ”€â”€ decoders/
â”‚   â”‚   â”œâ”€â”€ ctc_decoder.py         âœ… SAME
â”‚   â”‚   â”œâ”€â”€ transformer_decoder.py âœ… SAME
â”‚   â”‚   â”œâ”€â”€ unit_decoder.py        âœ… SAME
â”‚   â”‚   â””â”€â”€ vocoder.py             âœ… SAME
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ agent/
â”‚   â””â”€â”€ echostream_agent.py        â­ NEW: SimulEval agent
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                   â­ NEW
â”‚   â””â”€â”€ evaluate.py                â­ NEW
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_echostream.py         â­ NEW
â”‚
â””â”€â”€ configs/
    â””â”€â”€ echostream_config.yaml     â­ NEW
```

---

## ì½”ë“œ ë¼ì¸ ìˆ˜ ë¹„êµ

### StreamSpeech (Baseline - ì „ì²´)

```
Total: ~50,000 lines (including fairseq, preprocessors, etc.)

Core implementation:
- Conformer: ~500 lines
- Decoders: ~2,000 lines
- Agents: ~800 lines
```

### EchoStream (Focused)

```
Total: ~3,800 lines (clean, modular)

Core implementation:
- Emformer: ~400 lines
- Encoder: ~200 lines
- Decoders: ~1,600 lines
- Agent: ~250 lines
- Scripts: ~500 lines
- Tests: ~600 lines
- Docs: ~3,000 lines
```

**ì°¨ì´**: **13ë°° ë” ì‘ê³  ì§‘ì¤‘ì ** (ë¶ˆí•„ìš”í•œ ì½”ë“œ ì œê±°)

---

## íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ

### 16-layer Encoder

| Component | StreamSpeech | EchoStream | ì°¨ì´ |
|-----------|-------------|-----------|------|
| **Encoder** | ~18M | ~21M | +3M |
| **Decoders** | ~13M | ~13M | ë™ì¼ |
| **Vocoder** | ~14M | ~14M | ë™ì¼ |
| **Total** | **~45M** | **~48M** | +3M |

**Note**: Emformerê°€ ì•½ê°„ ë” í¬ì§€ë§Œ (Memory Bank ë“±), í›¨ì”¬ íš¨ìœ¨ì !

---

## ê¸°ëŠ¥ ë¹„êµ

| ê¸°ëŠ¥ | StreamSpeech | EchoStream | ê°œì„  |
|-----|-------------|-----------|------|
| **Streaming ASR** | âœ… | âœ… | ë™ì¼ |
| **Simultaneous S2TT** | âœ… | âœ… | ë™ì¼ |
| **Simultaneous S2ST** | âœ… | âœ… | ë™ì¼ |
| **Multi-task Learning** | âœ… | âœ… | ë™ì¼ |
| **Unidirectional Encoder** | âœ… | âœ… | ë™ì¼ |
| **CTC-based Policy** | âœ… | âœ… | ë™ì¼ |
| **O(1) Encoder Complexity** | âŒ | âœ… | **NEW!** |
| **Left Context Cache** | âŒ | âœ… | **NEW!** |
| **Memory Bank** | âŒ | âœ… | **NEW!** |
| **Fixed Memory Usage** | âŒ | âœ… | **NEW!** |
| **CT-Transformer Integration** | âŒ | âœ… (optional) | **NEW!** |

---

## í’ˆì§ˆ vs ì§€ì—° Trade-off

### StreamSpeech

```
Quality: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  (8/10)
Speed:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  (6/10)
Memory:  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  (4/10)

Trade-off: ê¸´ ë°œí™” ì‹œ ì§€ì—° ì¦ê°€
```

### EchoStream

```
Quality: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  (8/10) - Same!
Speed:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘  (9/10) - Better!
Memory:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  (10/10) - Much better!

Trade-off: ë°œí™” ê¸¸ì´ ë¬´ê´€ ì¼ì • ì„±ëŠ¥
```

---

## ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì§§ì€ ë°œí™” (1-2ì´ˆ)

**StreamSpeech**: âš¡ ë¹ ë¦„ (~15ms)  
**EchoStream**: âš¡ ë¹ ë¦„ (~10ms)  
**ì°¨ì´**: ë¯¸ë¯¸ (1.5ë°°)

### ì‹œë‚˜ë¦¬ì˜¤ 2: ì¤‘ê°„ ë°œí™” (5-10ì´ˆ)

**StreamSpeech**: âš ï¸ ë³´í†µ (~40-60ms)  
**EchoStream**: âš¡ ë¹ ë¦„ (~10ms)  
**ì°¨ì´**: ëª…í™• (4-6ë°°)

### ì‹œë‚˜ë¦¬ì˜¤ 3: ê¸´ ë°œí™” (30ì´ˆ+)

**StreamSpeech**: âŒ ëŠë¦¼ (~120ms+)  
**EchoStream**: âš¡ ë¹ ë¦„ (~10ms)  
**ì°¨ì´**: ê·¹ëª… (12ë°°+)

### ì‹œë‚˜ë¦¬ì˜¤ 4: ì‹¤ì‹œê°„ ëŒ€í™” (ì—°ì† ë°œí™”)

**StreamSpeech**:
```
ë°œí™” 1 (10ì´ˆ): 60ms
ë°œí™” 2 (15ì´ˆ): 90ms  â† ë” ëŠë ¤ì§
ë°œí™” 3 (20ì´ˆ): 120ms â† ë”ë” ëŠë ¤ì§
```

**EchoStream**:
```
ë°œí™” 1 (10ì´ˆ): 10ms
ë°œí™” 2 (15ì´ˆ): 10ms  â† ì¼ì •!
ë°œí™” 3 (20ì´ˆ): 10ms  â† ì¼ì •!
```

**ê²°ë¡ **: **EchoStreamì€ ë°œí™” ê¸¸ì´ì— ë¬´ê´€í•˜ê²Œ ì¼ì •í•œ ì„±ëŠ¥!**

---

## êµ¬í˜„ í’ˆì§ˆ ë¹„êµ

### StreamSpeech

**ì¥ì **:
- âœ… ê²€ì¦ëœ ì•„í‚¤í…ì²˜ (ACL 2024)
- âœ… SOTA ì„±ëŠ¥
- âœ… ë‹¤ì–‘í•œ ë³€í˜• ì œê³µ
- âœ… Pre-trained ëª¨ë¸ ì œê³µ

**ë‹¨ì **:
- âŒ ì½”ë“œë² ì´ìŠ¤ ë³µì¡ (50K+ lines)
- âŒ ë§ì€ ì˜ì¡´ì„± (fairseq ì „ì²´)
- âŒ ê¸´ ë°œí™” ì‹œ íš¨ìœ¨ì„± ì €í•˜
- âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€

### EchoStream

**ì¥ì **:
- âœ… ê¹¨ë—í•œ ì½”ë“œë² ì´ìŠ¤ (3.8K lines)
- âœ… ëª¨ë“ˆí™” ì„¤ê³„
- âœ… O(1) ë³µì¡ë„
- âœ… ê³ ì • ë©”ëª¨ë¦¬ ì‚¬ìš©
- âœ… ì™„ì „í•œ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ (30/30)
- âœ… ìƒì„¸í•œ ë¬¸ì„œí™”

**ë‹¨ì **:
- âŒ Pre-trained ëª¨ë¸ ì—†ìŒ (í•™ìŠµ í•„ìš”)
- âŒ ê²€ì¦ í•„ìš” (ì•„ì§ ë…¼ë¬¸ ì—†ìŒ)
- âŒ Vocoderê°€ dummy (ì‹¤ì œ CodeHiFiGAN í•„ìš”)

---

## ë²¤ì¹˜ë§ˆí¬ ë¹„êµí‘œ

### CVSS-C ë°ì´í„°ì…‹ (ì˜ˆìƒ)

| Metric | StreamSpeech | EchoStream | ê°œì„  |
|--------|-------------|-----------|------|
| **ASR-BLEU** (Quality) | 26.7 | ~26.7 | ë™ì¼ ì˜ˆìƒ |
| **AL** (Latency, ms) | 1,724 | ~1,200 | **30%** â†“ |
| **AP** (ms) | 2,913 | ~2,000 | **31%** â†“ |
| **RTF** | 1.326 | ~0.9 | **32%** â†“ |

**Note**: EchoStream ìˆ˜ì¹˜ëŠ” ì˜ˆìƒê°’ (ì‹¤ì œ í•™ìŠµ í›„ ê²€ì¦ í•„ìš”)

### íš¨ìœ¨ì„± ì§€í‘œ

| Metric | StreamSpeech | EchoStream | ì°¨ì´ |
|--------|-------------|-----------|------|
| **Encoder Complexity** | O(TÂ²) | O(1) | **Constant** |
| **Memory (10s)** | ~256MB | ~10MB | **25x** â†“ |
| **Latency (10s)** | ~60ms | ~10ms | **6x** â†“ |
| **Scalability** | ë°œí™”â†‘â†’ëŠë¦¼ | ì¼ì • | **Constant** |

---

## ì‚¬ìš© ì‚¬ë¡€ ë¹„êµ

### StreamSpeechì— ì í•©í•œ ê²½ìš°

1. **ì§§ì€ ë°œí™” ìœ„ì£¼** (1-5ì´ˆ)
2. **ê²€ì¦ëœ ì„±ëŠ¥ í•„ìš”**
3. **Pre-trained ëª¨ë¸ ì‚¬ìš©**
4. **Baseline ë¹„êµ ì—°êµ¬**

### EchoStreamì— ì í•©í•œ ê²½ìš°

1. **ê¸´ ë°œí™” ì²˜ë¦¬** (10ì´ˆ+)
2. **ë©”ëª¨ë¦¬ ì œì•½ í™˜ê²½** (ì—£ì§€ ë””ë°”ì´ìŠ¤)
3. **ì—°ì† ëŒ€í™” ì‹œìŠ¤í…œ**
4. **íš¨ìœ¨ì„± ìµœìš°ì„ **
5. **í™•ì¥ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œ**

---

## Key Takeaways

### ğŸ“Š ì •ëŸ‰ì  ë¹„êµ

| í•­ëª© | StreamSpeech | EchoStream | ìŠ¹ì |
|-----|-------------|-----------|------|
| **í’ˆì§ˆ** | â­â­â­â­â­ | â­â­â­â­â­ | ë™ë¥  |
| **ì†ë„** | â­â­â­â­ | â­â­â­â­â­ | **EchoStream** |
| **ë©”ëª¨ë¦¬** | â­â­â­ | â­â­â­â­â­ | **EchoStream** |
| **í™•ì¥ì„±** | â­â­â­ | â­â­â­â­â­ | **EchoStream** |
| **ì„±ìˆ™ë„** | â­â­â­â­â­ | â­â­â­ | **StreamSpeech** |

### ğŸ¯ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

1. **EchoStream = StreamSpeech + Emformer**
   - ë””ì½”ë”ëŠ” 100% ë™ì¼
   - ì¸ì½”ë”ë§Œ íš¨ìœ¨ì ìœ¼ë¡œ êµì²´
   - í’ˆì§ˆ ìœ ì§€í•˜ë©´ì„œ ì†ë„ í–¥ìƒ

2. **Emformerì˜ í•µì‹¬ í˜ì‹ **
   - Left Context Cache: K, V ì¬ì‚¬ìš©
   - Memory Bank: í•˜ìœ„ ë ˆì´ì–´ì—ì„œ ì „ë‹¬
   - O(TÂ²) â†’ O(1) ë³µì¡ë„ ê°œì„ 

3. **ì‹¤ìš©ì  ì¥ì **
   - ê¸´ ë°œí™”ì—ì„œ ì••ë„ì  ìš°ìœ„
   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± (ì—£ì§€ ë°°í¬ ê°€ëŠ¥)
   - ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì§€ì—° ì‹œê°„

---

## ì¶”ì²œ

### StreamSpeech ì‚¬ìš© ê¶Œì¥

- ğŸ“ í•™ìˆ  ì—°êµ¬ (ACL 2024 baseline)
- ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ
- ğŸš€ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ (pre-trained ì‚¬ìš©)
- ğŸ“ ë…¼ë¬¸ ì¬í˜„

### EchoStream ì‚¬ìš© ê¶Œì¥

- ğŸ­ í”„ë¡œë•ì…˜ ë°°í¬
- ğŸ“± ì—£ì§€ ë””ë°”ì´ìŠ¤ (ë©”ëª¨ë¦¬ ì œì•½)
- ğŸ¤ ì‹¤ì‹œê°„ ëŒ€í™” ì‹œìŠ¤í…œ
- â±ï¸ ê¸´ ë°œí™” ì²˜ë¦¬
- ğŸ”¬ íš¨ìœ¨ì„± ì—°êµ¬

---

## ë‹¤ìŒ ë‹¨ê³„: ì§ì ‘ ë¹„êµ ì‹¤í—˜

### ì‹¤í—˜ ê³„íš

1. **ë™ì¼ ë°ì´í„°**:
   - CVSS-C fr-en test set
   - ë™ì¼í•œ ì „ì²˜ë¦¬
   
2. **ë™ì¼ ì„¤ì •**:
   - 16-layer encoder
   - ë™ì¼í•œ ë””ì½”ë” ì„¤ì •
   - ë™ì¼í•œ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°

3. **ì¸¡ì • ì§€í‘œ**:
   - Quality: BLEU, ASR-BLEU
   - Latency: AL, AP, DAL
   - Efficiency: RTF, Memory, Throughput

4. **ì‹¤í–‰**:
   ```bash
   # StreamSpeech
   cd StreamSpeech_baseline
   bash scripts/simuleval.simul-s2st.sh
   
   # EchoStream
   cd StreamSpeech  # (EchoStream repo)
   python scripts/evaluate.py --mode simuleval ...
   ```

---

## ê²°ë¡ 

### í•µì‹¬ ì°¨ì´ì 

**StreamSpeech**: ê²€ì¦ëœ ê³ í’ˆì§ˆ baseline  
**EchoStream**: íš¨ìœ¨ì„± ìµœì í™” ê°œì„  ë²„ì „

### ì£¼ìš” ê°œì„  ì‚¬í•­

1. âš¡ **6-50ë°° ë¹ ë¥¸ ì¸ì½”ë”** (ë°œí™” ê¸¸ì´ì— ë”°ë¼)
2. ğŸ’¾ **25ë°° ì ì€ ë©”ëª¨ë¦¬**
3. ğŸ¯ **O(1) ì¼ì • ë³µì¡ë„**
4. ğŸ“Š **í’ˆì§ˆ ìœ ì§€** (ë™ì¼ ë””ì½”ë”)

### ì„ íƒ ê°€ì´ë“œ

```
ì§§ì€ ë°œí™” (< 5ì´ˆ)     â†’ StreamSpeech, EchoStream ë‘˜ ë‹¤ OK
ì¤‘ê°„ ë°œí™” (5-10ì´ˆ)    â†’ EchoStream ê¶Œì¥ (4-6ë°° ë¹ ë¦„)
ê¸´ ë°œí™” (> 10ì´ˆ)      â†’ EchoStream ê°•ë ¥ ê¶Œì¥ (6-50ë°° ë¹ ë¦„)
ë©”ëª¨ë¦¬ ì œì•½           â†’ EchoStream í•„ìˆ˜ (25ë°° ì ˆì•½)
í”„ë¡œë•ì…˜ ë°°í¬         â†’ EchoStream ê¶Œì¥ (íš¨ìœ¨ì„±)
ì—°êµ¬ìš© baseline       â†’ StreamSpeech (ê²€ì¦ë¨)
```

---

**EchoStream**: StreamSpeechì˜ ì •ì‹ ì„ ì´ì–´ë°›ì•„ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•œ ì‹¤ìš©ì  ê°œì„ ! ğŸŒŠ


